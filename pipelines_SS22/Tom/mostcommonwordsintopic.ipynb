{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b7fe89-badd-449f-afc8-ada486a4b8a5",
   "metadata": {},
   "source": [
    "### most common words in topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2731b495-dabe-407f-b9be-9a9ad349e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local utilities\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utilities.BTTools import filter_for\n",
    "from utilities.BTTools import groupSpeechesByDiscussionTitle\n",
    "\n",
    "# other stuff needed\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "with open('../../data/speeches_20.jsonl', 'r', encoding='utf8') as fp:\n",
    "    data = list(fp)\n",
    "speeches_original = []\n",
    "for line in data:\n",
    "    speeches_original.append(json.loads(line))\n",
    "    \n",
    "speeches = speeches_original.copy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20231eae-78e2-450e-8c61-6f789d90161f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedby_discussion = groupSpeechesByDiscussionTitle(speeches)\n",
    "len(groupedby_discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f0e1dda-5920-4ab3-a43f-3ea674f91354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for top in groupedby_discussion.values():\n",
    "    #print(grouped[top])\n",
    "    aggregate= []\n",
    "    for ele in top:\n",
    "        aggregate.append(ele['text'])\n",
    "    corpus.append(' '.join(aggregate))\n",
    "\n",
    "len(corpus)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ff6c6d7-3f7f-4a22-b186-f33efe9bd397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Illing\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\Tom Illing\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<208x19567 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 300671 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(max_df=0.8, min_df=0.01,lowercase = False)\n",
    "tf_idf_matrix  = vectorizer_tfidf.fit_transform(corpus)\n",
    "feature_names = vectorizer_tfidf.get_feature_names()\n",
    "vectorizer_tf = TfidfVectorizer(vocabulary=feature_names,use_idf=False, norm=\"l1\")\n",
    "#vectorizer_counts = CountVectorizer(vocabulary=feature_names)\n",
    "tf_matrix = vectorizer_tf.fit_transform(corpus)\n",
    "\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb43ca8e-b65f-45a7-b052-4af9ab670805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Energien Gas erneuerbaren EEG Ausbau Strom Umlage Energieversorgung Energiewende LNG\n",
      "Topic 1: Ukraine Russland Krieg NATO Putin Europa Waffen Bundeskanzler russischen Sanktionen\n",
      "Topic 2: Impfpflicht Impfung Pandemie impfen Omikron Pflege Impfen geimpft Variante Impfungen\n",
      "Topic 3: Euro Milliarden Haushalt Sondervermögen Nachtragshaushalt Investitionen Pandemie Wirtschaft Unternehmen Bundeswehr\n",
      "Topic 4: Mali Mandat Soldaten Soldatinnen Bundeswehr Mission Südsudan Libyen Sea MINUSMA\n",
      "Topic 5: Wohnungen Bauen Wohnen Mieter Vorkaufsrecht Mieterinnen Wohnraum CO2 Wohnungsbau Kommunen\n",
      "Topic 6: Landwirtschaft Landwirte Hunger Ernährung Bauern Lebensmittel Millionen Flächen Welt Ukraine\n",
      "Topic 7: Inflation Euro Einkommen Entlastung Antrag entlasten Progression Preise Rentner Energiepreispauschale\n",
      "Topic 8: Antrag Demokratie Opfer Sicherheitsbehörden Hanau Rechtsextremismus Frauen 219a Antidiskriminierungsstelle Rechtsstaat\n",
      "Topic 9: Kinder Kommunen Ganztagsbetreuung Frauen Schulen Bund Bildung Familien Länder Ministerin\n"
     ]
    }
   ],
   "source": [
    "# Use NMF to look for 15 topics\n",
    "n_topics = 10\n",
    "model = NMF(n_components=n_topics)\n",
    "model.fit(tf_idf_matrix)\n",
    "#model.fit(tf_matrix)\n",
    "\n",
    "# Print the top 10 words\n",
    "n_words = 10\n",
    "n_words_features = 100\n",
    "\n",
    "topic_list = []\n",
    "topic_list_ext = []\n",
    "topic_words = []\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    top_n = [feature_names[i]\n",
    "             for i in topic.argsort()\n",
    "             [-n_words:]][::-1]\n",
    "    top_features = ' '.join(top_n)\n",
    "    topic_list.append(f\"topic_{'_'.join(top_n[:3])}\") \n",
    "    topic_list_ext.append(top_features)\n",
    "    \n",
    "    top_n = [feature_names[i]\n",
    "             for i in topic.argsort()\n",
    "             [-n_words_features:]][::-1]\n",
    "    topic_words.append(top_n)\n",
    "\n",
    "    print(f\"Topic {topic_idx}: {top_features}\")\n",
    "    \n",
    "topic2word = model.components_   \n",
    "#topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7009039d-2712-4e06-9fc4-4d8dc335dc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Energien Gas erneuerbaren EEG Ausbau Strom Umlage Energieversorgung Energiewende LNG',\n",
       " 'Ukraine Russland Krieg NATO Putin Europa Waffen Bundeskanzler russischen Sanktionen',\n",
       " 'Impfpflicht Impfung Pandemie impfen Omikron Pflege Impfen geimpft Variante Impfungen',\n",
       " 'Euro Milliarden Haushalt Sondervermögen Nachtragshaushalt Investitionen Pandemie Wirtschaft Unternehmen Bundeswehr',\n",
       " 'Mali Mandat Soldaten Soldatinnen Bundeswehr Mission Südsudan Libyen Sea MINUSMA',\n",
       " 'Wohnungen Bauen Wohnen Mieter Vorkaufsrecht Mieterinnen Wohnraum CO2 Wohnungsbau Kommunen',\n",
       " 'Landwirtschaft Landwirte Hunger Ernährung Bauern Lebensmittel Millionen Flächen Welt Ukraine',\n",
       " 'Inflation Euro Einkommen Entlastung Antrag entlasten Progression Preise Rentner Energiepreispauschale',\n",
       " 'Antrag Demokratie Opfer Sicherheitsbehörden Hanau Rechtsextremismus Frauen 219a Antidiskriminierungsstelle Rechtsstaat',\n",
       " 'Kinder Kommunen Ganztagsbetreuung Frauen Schulen Bund Bildung Familien Länder Ministerin']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_list_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04847a0c-ca8c-4614-8138-d321b0dfa2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 19567)\n",
      "(10, 19567)\n",
      "(208, 10)\n"
     ]
    }
   ],
   "source": [
    "print(tf_matrix.shape)\n",
    "print(topic2word.shape)\n",
    "doc2topic = (tf_matrix * topic2word.T)\n",
    "print(doc2topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df67e674-1ba5-489b-ac6b-b89a94666a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0324893 , 0.02277637, 0.04251865, 0.02831381, 0.03472841,\n",
       "       0.10979025, 0.03810056, 0.035589  , 0.03160826, 0.02825281])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic2word.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8d1d65f-3a3d-40fd-9bac-3d07e0950cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_topic_num = []\n",
    "feature_topic_name = []\n",
    "for wx,word in enumerate(feature_names):\n",
    "    if topic2word.T[wx].max() > 0:\n",
    "        topic_num = topic2word.T[wx].argmax()\n",
    "        topic_name = topic_list_ext[topic_num]\n",
    "    else:\n",
    "        topic_num = n_topics\n",
    "        topic_name = 'not defined'\n",
    "    \n",
    "    feature_topic_num.append(topic_num)\n",
    "    feature_topic_name.append(topic_name)\n",
    "#feature_topic_num\n",
    "\n",
    "top_topic_num = []\n",
    "top_topic_name = []\n",
    "for tx,top in enumerate(groupedby_discussion):\n",
    "    if doc2topic[tx].max() > 0:\n",
    "        topic_num = doc2topic[tx].argmax()\n",
    "        topic_name = topic_list_ext[topic_num]\n",
    "    else:\n",
    "        topic_num = n_topics\n",
    "        topic_name = 'not defined'\n",
    "    \n",
    "    top_topic_num.append(topic_num)\n",
    "    top_topic_name.append(topic_name)\n",
    "    \n",
    "#feature_topic_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7999d17-3125-46ab-9a2c-97b341a51fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 has 152 TOPs\n",
      "(152, 152)\n",
      "This graph has 152 nodes and 1562 links.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './TOPnets/TOPnet4topic0.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m d3graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     96\u001b[0m htmlcode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<head>\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124m    <style> body \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124mmargin: 0;\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m </style>\u001b[39m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124m    <script src=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://unpkg.com/force-graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m></script>\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124m</body>\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./TOPnets/TOPnet4topic\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtopic_index\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.html\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    124\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(htmlcode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './TOPnets/TOPnet4topic0.html'"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_index = 0\n",
    "\n",
    "for topic_index, topic_name in enumerate(topic_list):\n",
    "\n",
    "    # select TOPs for max. relevant topic\n",
    "    #topic_selection = []\n",
    "    #for tx,top in enumerate(groupedby_discussion):\n",
    "    #    if top_topic_num[tx] == topic_index:\n",
    "    #        topic_selection.append(groupedby_discussion[top])\n",
    "    #        #print(top)\n",
    "\n",
    "    # Alternative using the corpus\n",
    "    topic_selection = []\n",
    "    top_indices = []\n",
    "    top_names = []\n",
    "    for tx,top in enumerate(corpus):\n",
    "        if top_topic_num[tx] == topic_index:\n",
    "            topic_selection.append(top)\n",
    "            top_indices.append(tx)\n",
    "            top_names.append(list(groupedby_discussion.keys())[tx])\n",
    "            #print(top)\n",
    "    \n",
    "    print(f'Topic {topic_index} has {len(topic_selection)} TOPs')\n",
    "    \n",
    "    # remove words that are max. relevant for topic        \n",
    "    topic_features = []\n",
    "    for wx,word in enumerate(feature_names):\n",
    "        if feature_topic_num[wx] != topic_index:\n",
    "            topic_features.append(word)\n",
    "\n",
    "    #len(topic_features)\n",
    "\n",
    "    # compute semantic similarity for selection\n",
    "    if len(topic_selection) == 0:\n",
    "        print(f'Topic {topic_index} cannot be constructed')\n",
    "    else:\n",
    "        vectorizer_topic = TfidfVectorizer(vocabulary=topic_features,lowercase = False)\n",
    "        tf_idf_matrix_topic  = vectorizer_topic.fit_transform(topic_selection)\n",
    "        pairwise_similarity_topic = tf_idf_matrix_topic * tf_idf_matrix_topic.T \n",
    "        similarity_topic = pairwise_similarity_topic.toarray()\n",
    "\n",
    "        print(similarity_topic.shape)\n",
    "\n",
    "\n",
    "        # build network\n",
    "        # nodes:\n",
    "        nodes = []\n",
    "        count = 1\n",
    "        for tx,top_ix in enumerate(top_indices):\n",
    "            node_dict = {\n",
    "                'id' : count,\n",
    "                'top' : top_ix,\n",
    "                'date' : groupedby_discussion[ top_names[tx] ][0]['date'],\n",
    "                'nReden' : len( groupedby_discussion[ top_names[tx] ] )\n",
    "            }\n",
    "            nodes.append(node_dict)\n",
    "            count += 1\n",
    "\n",
    "        # graph construct\n",
    "        graph = {\n",
    "            'directed': False,\n",
    "            'graph': 'semant_graph',\n",
    "            'links': [],\n",
    "            'nodes': nodes,\n",
    "        }   \n",
    "\n",
    "        # edges:\n",
    "        min_weight = 0.15\n",
    "        for ix,nodeI in enumerate(graph['nodes']):\n",
    "            for jx,nodeJ in enumerate(graph['nodes']):\n",
    "                if ix < jx:\n",
    "                    source = nodeI['id']\n",
    "                    target = nodeJ['id']\n",
    "                    weight = similarity_topic[ix,jx]\n",
    "                    if weight > min_weight:\n",
    "                        link_dict = {\n",
    "                            'source':source,\n",
    "                            'target':target,\n",
    "                            'weight':weight       \n",
    "                        }\n",
    "                        graph['links'].append(link_dict)\n",
    "\n",
    "\n",
    "        nn = len(graph['nodes'])\n",
    "        ne = len(graph['links'])\n",
    "        print( f\"This graph has {nn} nodes and {ne} links.\")\n",
    "\n",
    "\n",
    "        # write to HTML\n",
    "\n",
    "        data = graph\n",
    "        d3graph = {\"nodes\": [], \"links\": []}\n",
    "        d3graph[\"nodes\"] = data[\"nodes\"]\n",
    "        d3graph[\"links\"] = data[\"links\"]\n",
    "\n",
    "        htmlcode = f\"\"\"<head>\n",
    "            <style> body {{margin: 0;}} </style>\n",
    "            <script src=\"https://unpkg.com/force-graph\"></script>\n",
    "            <meta charset=\"UTF-8\">\n",
    "        </head>\n",
    "        <body>\n",
    "        <div id=\"graph\"></div>\n",
    "        <script>\n",
    "            var data = {d3graph};\n",
    "            const elem = document.getElementById('graph');\n",
    "            const Graph = ForceGraph()(elem)\n",
    "                .graphData(data)\n",
    "                .nodeLabel('top')\n",
    "                .nodeRelSize(1)\n",
    "                .nodeVal('nReden')\n",
    "                //.linkVisibility('true')\n",
    "                //.onNodeClick (node => {{window.open(`wordnet.html`, '_blank')}})\n",
    "                //.onNodeHover(node => elem.style.cursor = node ? 'pointer' : null)\n",
    "                .onNodeRightClick(node => {{\n",
    "                    // Center/zoom on node\n",
    "                    Graph.centerAt(node.x, node.y, 1000);\n",
    "                    Graph.zoom(4, 2000);\n",
    "                }});\n",
    "        </script>\n",
    "        </body>\n",
    "        \"\"\"\n",
    "\n",
    "        with open (f\"./TOPnets/TOPnet4topic{topic_index}.html\", \"w\") as f:\n",
    "            f.write(htmlcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfed8b4e-a7b7-4c47-bb73-ebf21dfa0142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Tagesordnungspunkt 2 2021-10-26', 'Tagesordnungspunkt 3 2021-10-26', 'Tagesordnungspunkt 5 2021-10-26', 'Tagesordnungspunkt 6 2021-10-26', 'Tagesordnungspunkt 1 2021-11-11', 'Tagesordnungspunkt 3 2021-11-11', 'Tagesordnungspunkt 4 2021-11-11', 'Tagesordnungspunkt 5 2021-11-11', 'Tagesordnungspunkt 6 2021-11-11', 'Tagesordnungspunkt 7 2021-11-11', 'Zusatzpunkt 2 2021-11-11', 'Tagesordnungspunkt 1 2021-11-18', 'Zusatzpunkt 1 2021-11-18', 'Tagesordnungspunkt 3 2021-11-18', 'Tagesordnungspunkt 4 2021-11-18', 'Tagesordnungspunkt 5 2021-11-18', 'Zusatzpunkt 2 2021-11-18', 'Tagesordnungspunkt 1 2021-12-07', 'Tagesordnungspunkt 2 2021-12-08', 'Tagesordnungspunkt 3 2021-12-08', 'Tagesordnungspunkt 5 2021-12-08', 'Tagesordnungspunkt 8 2021-12-09', 'Tagesordnungspunkt 9 2021-12-09', 'Tagesordnungspunkt 10 2021-12-09', 'Tagesordnungspunkt 11 2021-12-09', 'Zusatzpunkt 1 2021-12-09', 'Zusatzpunkt 2 2021-12-09', 'Tagesordnungspunkt 13 2021-12-10', 'Tagesordnungspunkt 1 2021-12-15', 'Tagesordnungspunkt 3 2021-12-16', 'Tagesordnungspunkt 4 2021-12-16', 'Tagesordnungspunkt 5 2021-12-16', 'Tagesordnungspunkt 6 2021-12-16', 'Tagesordnungspunkt 7 2021-12-16', 'Tagesordnungspunkt 8 2021-12-16', 'Tagesordnungspunkt 9 2021-12-16', 'Zusatzpunkt 2 2021-12-16', 'Zusatzpunkt 3 2021-12-16', 'Tagesordnungspunkt 1 2022-01-12', 'Tagesordnungspunkt 2 2022-01-12', 'Tagesordnungspunkt 2 2022-01-13', 'Zusatzpunkt 3 2022-01-13', 'Tagesordnungspunkt 2 2022-01-14', 'Zusatzpunkt 4 2022-01-14', 'Tagesordnungspunkt 1 2022-01-26', 'Tagesordnungspunkt 3 2022-01-26', 'Zusatzpunkt 1 2022-01-27', 'Tagesordnungspunkt 5 2022-01-27', 'Tagesordnungspunkt 6 2022-01-27', 'Tagesordnungspunkt 7 2022-01-27', 'Zusatzpunkt 2 2022-01-27', 'Tagesordnungspunkt 8 2022-01-27', 'Tagesordnungspunkt 9 2022-01-27', 'Tagesordnungspunkt 6 2022-04-07', 'Zusatzpunkt 2 2022-04-07', 'Tagesordnungspunkt 8 2022-04-07', 'Tagesordnungspunkt 9 2022-04-07', 'Tagesordnungspunkt 25 2022-04-07', 'Tagesordnungspunkt 11 2022-04-07', 'Tagesordnungspunkt 12 2022-04-07', 'Tagesordnungspunkt 13 2022-04-07', 'Tagesordnungspunkt 23 2022-04-07', 'Tagesordnungspunkt 15 2022-04-07', 'Tagesordnungspunkt 16 2022-04-07', 'Tagesordnungspunkt 14 2022-04-07', 'Tagesordnungspunkt 17 2022-04-07', 'Zusatzpunkt 5 2022-04-07', 'Tagesordnungspunkt 2 2022-02-16', 'Zusatzpunkt 1 2022-02-16', 'Tagesordnungspunkt 6 2022-02-17', 'Tagesordnungspunkt 7 2022-02-17', 'Tagesordnungspunkt 8 2022-02-17', 'Tagesordnungspunkt 9 2022-02-17', 'Tagesordnungspunkt 10 2022-02-17', 'Tagesordnungspunkt 11 2022-02-17', 'Tagesordnungspunkt 12 2022-02-17', 'Tagesordnungspunkt 13 2022-02-17', 'Tagesordnungspunkt 14 2022-02-17', 'Tagesordnungspunkt 15 2022-02-17', 'Tagesordnungspunkt 16 2022-02-17', 'Tagesordnungspunkt 17 2022-02-17', 'Zusatzpunkt 5 2022-02-17', 'Tagesordnungspunkt 27 2022-02-17', 'Tagesordnungspunkt 19 2022-02-18', 'Tagesordnungspunkt 20 2022-02-18', 'Tagesordnungspunkt 21 2022-02-18', 'Tagesordnungspunkt 22 2022-02-18', 'Tagesordnungspunkt 23 2022-02-18', 'Zusatzpunkt 11 2022-02-18', 'Tagesordnungspunkt 25 2022-02-18', 'Zusatzpunkt 13 2022-02-18', 'Tagesordnungspunkt 1 2022-02-27', 'Tagesordnungspunkt 1 2022-03-16', 'Tagesordnungspunkt 2 2022-03-16', 'Zusatzpunkt 3 2022-03-16', 'Zusatzpunkt 4 2022-03-16', 'Tagesordnungspunkt 4 2022-03-16', 'Zusatzpunkt 5 2022-03-16', 'Zusatzpunkt 6 2022-03-16', 'Tagesordnungspunkt 6 2022-03-17', 'Tagesordnungspunkt 7 2022-03-17', 'Tagesordnungspunkt 8 2022-03-17', 'Tagesordnungspunkt 9 2022-03-17', 'Zusatzpunkt 8 2022-03-17', 'Tagesordnungspunkt 11 2022-03-17', 'Tagesordnungspunkt 12 2022-03-17', 'Tagesordnungspunkt 13 2022-03-17', 'Tagesordnungspunkt 14 2022-03-17', 'Tagesordnungspunkt 15 2022-03-17', 'Zusatzpunkt 9 2022-03-17', 'Tagesordnungspunkt 17 2022-03-17', 'Tagesordnungspunkt 18 2022-03-17', 'Zusatzpunkt 12 2022-03-17', 'Tagesordnungspunkt 19 2022-03-17', 'Zusatzpunkt 14 2022-03-17', 'Tagesordnungspunkt 21 2022-03-18', 'Tagesordnungspunkt 22 2022-03-18', 'Tagesordnungspunkt 23 2022-03-18', 'Tagesordnungspunkt 24 2022-03-18', 'Tagesordnungspunkt 25 2022-03-18', 'Tagesordnungspunkt 26 2022-03-18', 'Zusatzpunkt 20 2022-03-18', 'Tagesordnungspunkt 1 2022-03-22', 'Einzelplan 08 2022-03-22', 'Einzelplan 25 2022-03-22', 'Einzelplan 16 2022-03-22', 'Einzelplan 12 2022-03-22', 'Einzelplan 4 2022-03-23', 'Einzelplan 5 2022-03-23', 'Einzelplan 14 2022-03-23', 'Einzelplan 23 2022-03-23', 'Einzelplan 9 2022-03-24', 'Einzelplan 15 2022-03-24', 'Einzelplan 7 2022-03-24', 'Einzelplan 6 2022-03-24', 'Einzelplan 10 2022-03-24', 'Einzelplan 30 2022-03-24', 'Tagesordnungspunkt 1 2022-03-25', 'Einzelplan 11 2022-03-25', 'Einzelplan 17 2022-03-25', 'Tagesordnungspunkt 3 2022-03-25', 'Tagesordnungspunkt 1 2022-04-06', 'Zusatzpunkt 1 2022-04-06', 'Tagesordnungspunkt 3 2022-04-06', 'Tagesordnungspunkt 4 2022-04-06', 'Tagesordnungspunkt 5 2022-04-06', 'Tagesordnungspunkt 19 2022-04-08', 'Tagesordnungspunkt 7 2022-04-08', 'Tagesordnungspunkt 21 2022-04-08', 'Tagesordnungspunkt 22 2022-04-08', 'Zusatzpunkt 3 2022-04-08', 'Tagesordnungspunkt 1 2022-04-27', 'Tagesordnungspunkt 2 2022-04-27', 'Zusatzpunkt 1 2022-04-27', 'Tagesordnungspunkt 4 2022-04-27', 'Tagesordnungspunkt 5 2022-04-27', 'Zusatzpunkt 2 2022-04-27', 'Zusatzpunkt 3 2022-04-28', 'Tagesordnungspunkt 13 2022-04-28', 'Tagesordnungspunkt 7 2022-04-28', 'Tagesordnungspunkt 10 2022-04-28', 'Zusatzpunkt 7 2022-04-28', 'Tagesordnungspunkt 9 2022-04-28', 'Tagesordnungspunkt 15 2022-04-28', 'Tagesordnungspunkt 12 2022-04-28', 'Tagesordnungspunkt 17 2022-04-28', 'Tagesordnungspunkt 14 2022-04-28', 'Tagesordnungspunkt 19 2022-04-28', 'Tagesordnungspunkt 16 2022-04-28', 'Tagesordnungspunkt 18 2022-04-28', 'Tagesordnungspunkt 20 2022-04-29', 'Tagesordnungspunkt 23 2022-04-29', 'Tagesordnungspunkt 6 2022-04-29', 'Tagesordnungspunkt 24 2022-04-29', 'Tagesordnungspunkt 22 2022-04-29', 'Tagesordnungspunkt 1 2022-05-11', 'Zusatzpunkt 1 2022-05-11', 'Tagesordnungspunkt 3 2022-05-11', 'Tagesordnungspunkt 4 2022-05-11', 'Tagesordnungspunkt 5 2022-05-11', 'Tagesordnungspunkt 6 2022-05-11', 'Tagesordnungspunkt 7 2022-05-12', 'Tagesordnungspunkt 8 2022-05-12', 'Tagesordnungspunkt 9 2022-05-12', 'Tagesordnungspunkt 10 2022-05-12', 'Zusatzpunkt 5 2022-05-12', 'Tagesordnungspunkt 12 2022-05-12', 'Tagesordnungspunkt 26 2022-05-12', 'Tagesordnungspunkt 14 2022-05-12', 'Tagesordnungspunkt 15 2022-05-12', 'Tagesordnungspunkt\\xa016 2022-05-12', 'Zusatzpunkt 7 2022-05-12', 'Tagesordnungspunkt 18 2022-05-12', 'Tagesordnungspunkt 19 2022-05-12', 'Tagesordnungspunkt 20 2022-05-12', 'Tagesordnungspunkt 21 2022-05-12', 'Tagesordnungspunkt 22 2022-05-12', 'Tagesordnungspunkt 23 2022-05-12', 'Tagesordnungspunkt 24 2022-05-12', 'Zusatzpunkt 8 2022-05-12', 'Zusatzpunkt 9 2022-05-12', 'Tagesordnungspunkt 25 2022-05-13', 'Tagesordnungspunkt 13 2022-05-13', 'Tagesordnungspunkt 27 2022-05-13', 'Zusatzpunkt 10 2022-05-13', 'Tagesordnungspunkt 29 2022-05-13', 'Zusatzpunkt 11 2022-05-13', 'Tagesordnungspunkt 31 2022-05-13'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedby_discussion.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac561db7-d021-4137-879d-51ca8d0e82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection=['Tagesordnungspunkt 2 2021-10-26', 'Tagesordnungspunkt 3 2021-10-26', 'Tagesordnungspunkt 5 2021-10-26', 'Tagesordnungspunkt 6 2021-10-26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f3fcf61-70aa-4a43-b605-74dae900b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata=[]\n",
    "for ele in selection:\n",
    "    top=groupedby_discussion[ele]\n",
    "    #print(top)\n",
    "    for rede in top:\n",
    "        #print(rede['text'])\n",
    "        mydata.extend(rede['text'].split())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "133c6218-d81e-4ac4-af70-7b2dac09f3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sehr', 'geehrter', 'Herr', 'Alterspräsident!', 'So']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58132171-ee29-446c-94fc-01f4c12785ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('die', 143),\n",
       " ('der', 127),\n",
       " ('und', 100),\n",
       " ('das', 74),\n",
       " ('wir', 68),\n",
       " ('in', 65),\n",
       " ('ist', 54),\n",
       " ('für', 49),\n",
       " ('dass', 49),\n",
       " ('zu', 46),\n",
       " ('–', 43),\n",
       " ('auch', 42),\n",
       " ('es', 41),\n",
       " ('Sie', 41),\n",
       " ('nicht', 36)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "counter = collections.Counter(mydata).most_common()[:15]\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8f4c0-90cf-4014-adae-1b1a778a6dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
