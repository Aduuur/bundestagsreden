{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren der benötigten packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "#import collections\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laden der bereits gereinigten Daten aus einer JSON-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/speeches_preprocessed.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alleReden = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reden_clean = alleReden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellt eine Liste aller Parlamentarier, sowie ihrer dazugehörigen Parteien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_names_and_parties(reden_clean):\n",
    "    names = []\n",
    "    parties = []\n",
    "    for rede in reden_clean:\n",
    "        if rede['name'] not in names:\n",
    "            names.append(rede['name'])\n",
    "            parties.append(rede['party'])\n",
    "    return names, parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, parties = get_names_and_parties(reden_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erzeugt eine liste von dicts, welche jeweils einen der Parlamentarier enthalten. In den jeweiligen dicts sind außerdem ein Index, und die Parteizugehörigkeit des jeweiligen Parlamentariers gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_clean(name, reden):\n",
    "    text = ''\n",
    "    nReden = 0\n",
    "    for rede in reden:\n",
    "        if rede['name'] == name:\n",
    "            nReden += 1\n",
    "            text += ' '.join(rede['text_lem'])\n",
    "    return text, nReden\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_von_parla_mit_dict_text(reden, names, parties):\n",
    "    \n",
    "    parlamentarier = []\n",
    "\n",
    "    for count,name in enumerate(names):\n",
    "        hilf = {\n",
    "            'id':count+1,\n",
    "            'name': names[count],\n",
    "            'party': parties[count]\n",
    "        }\n",
    "        \n",
    "        text, nReden = get_text_clean(name, reden) \n",
    "            \n",
    "        hilf.update({'text_lem': text, 'nReden': nReden})\n",
    "        \n",
    "        parlamentarier.append(hilf)\n",
    "    \n",
    "    return parlamentarier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parlamentarier = liste_von_parla_mit_dict_text(reden_clean, names, parties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellt eine Matrix, welche als Zeilen die einzelnen Parlamentarier enthält. Die Zeilen sind gegeben durch die von allen Rednern gesagten Worte. \n",
    "Die einzelnen Eniträge sind die tf-idf Gewichte des von einem Parlamentarier gesagten Wortes.\n",
    "Die Berechnung der tfidf erfolgt folgerndermaßen:\n",
    "\n",
    "tf-idf(t, d) = tf(t, d) * idf(t)\n",
    "\n",
    "tf(t,d) = termfrequency des Terms t im Dokument d\n",
    "\n",
    "idf(t,d) = log [ (1 + n) / (1 + df(t)) ] + 1 inverse \n",
    "\n",
    "wobei df(t) die document frequency eines Wortes ist, also in wie vielen Worten eine Dokument vorkommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [ parla['text_lem'] for parla in parlamentarier ]\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=(2/781))\n",
    "tf_idf_matrix  = vectorizer.fit_transform(corpus)\n",
    "\n",
    "features = vectorizer.get_feature_names()\n",
    "print(f'We now consider {len(features)} different words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "erstelllt eine liste von dicts, wobei jedes dict folgende Info enthält:\n",
    "        - index\n",
    "        \n",
    "        - name des parlamentariers\n",
    "        \n",
    "        - Partei des Parlamentareirs\n",
    "        \n",
    "        - Anzahl der gehaltenen Reden\n",
    "        \n",
    "        - lemmatisiter Text aus allen Reden deíe der Parlamentarier gehalten hat\n",
    "        \n",
    "        - vec_number vektor der tfidf gewichte aus der tfidf matrix\n",
    "        \n",
    "        - msw ist das wort, welches das höchste gewicht im tfidf vetkro erhalten hat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_von_parla_mit_dict_vec(reden, parlamentarier, X_csr, vectorizer):\n",
    "    \n",
    "    parlamentarier_vec = []\n",
    "\n",
    "    for count, parla in enumerate(parlamentarier):\n",
    "        hilf = {\n",
    "            'id':count+1,\n",
    "            'name': parla['name'],\n",
    "            'party': parla['party'],\n",
    "            'nReden': parla['nReden'],\n",
    "            'text': parla['text_lem']\n",
    "        }\n",
    "        \n",
    "        vec_numbers = np.array(X_csr.getrow(count).toarray()[0])\n",
    "        \n",
    "        maxWX = np.argmax(vec_numbers)\n",
    "          \n",
    "        #vec_numbers = vec_numbers/np.linalg.norm(vec_numbers)\n",
    "        \n",
    "        hilf.update({'vec_numbers': vec_numbers})\n",
    "        msw = list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(np.argmax(vec_numbers))]\n",
    "        hilf.update({'maxTFIDF': msw})\n",
    "        \n",
    "        \n",
    "        parlamentarier_vec.append(hilf)\n",
    "    \n",
    "    return parlamentarier_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parlamentarier_vec = liste_von_parla_mit_dict_vec(reden_clean, parlamentarier, tf_idf_matrix.copy(), vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnet die Cosinusähnlichkeit zweier Parlamentarier anhand des Skalarproduktes zweier Parlamentarier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_similarity = tf_idf_matrix * tf_idf_matrix.T \n",
    "\n",
    "similarity = pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellt den Graphen. Die Knoten sind die Parlamentarier und das Gewicht ist die semantische Ähnlichkeit zweier Parlamentarier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vec_a, vec_b):\n",
    "    \n",
    "    return np.dot(vec_a, vec_b) /(np.linalg.norm(vec_a) * np.linalg.norm(vec_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cotop_graph_erstellen(parlamentarier, min_weight, similarity):\n",
    "\n",
    "    graph = {\n",
    "        'directed': False,\n",
    "        'graph': 'semant_graph',\n",
    "        'links': [],\n",
    "        'nodes': parlamentarier,\n",
    "    }\n",
    "\n",
    "    for ix,nodeI in enumerate(graph['nodes']):\n",
    "        for jx,nodeJ in enumerate(graph['nodes']):\n",
    "            if nodeI['id'] < nodeJ['id']:\n",
    "                source = nodeI['id']\n",
    "                target = nodeJ['id']\n",
    "                #weight = cos_sim(nodeI['vec_numbers'], nodeJ['vec_numbers'])\n",
    "                #r = np.corrcoef(nodeI['vec_numbers'], nodeJ['vec_numbers'])\n",
    "                #weight = r[0,1]\n",
    "                weight = similarity[ix,jx]\n",
    "                if weight > min_weight:\n",
    "                    link_dict = {\n",
    "                        'source':source,\n",
    "                        'target':target,\n",
    "                        'weight':weight       \n",
    "                    }\n",
    "                    graph['links'].append(link_dict)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = cotop_graph_erstellen(parlamentarier_vec, 0, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = len(graph['nodes'])\n",
    "ne = len(graph['links'])\n",
    "print( f\"This graph has {nn} nodes and {ne} links.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weights=[]\n",
    "for link in graph['links']:\n",
    "    weights.append(link['weight'])\n",
    "#print(sum(weights))\n",
    "\n",
    "plt.hist(weights, bins=25)\n",
    "plt.title(\"Distribution of Weights\")\n",
    "plt.xlabel(\"Wert\")\n",
    "plt.ylabel(\"Häufigkeit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Überführt den Graphen in einen Graphen, der mithilfe von Gephi verarbeitet werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graphforgephi = nx.Graph()\n",
    "for node in graph['nodes']:\n",
    "    graphforgephi.add_node(node['id'],name = node['name'],party = node['party'],msw = node['maxTFIDF'], nReden = node['nReden']);\n",
    "    #if(node['name'] == 'Angela Merkel'):\n",
    "    #    print(node['tops'])\n",
    "\n",
    "print('nodes done')\n",
    "for link in graph['links']:\n",
    "    if link['weight'] >= 0:#.22285057620424778:\n",
    "        graphforgephi.add_edge(link['source'],link['target'],weight = link['weight'])\n",
    "print('links done') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnet die vorhandenen Nachbarschaften im Graphen anhand des Louivain-Algorithmusses und fügt sie den Knoten hinzu. (Evtl. muss folgende Package installiert werden: pip install python-louvain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community as community_louvain\n",
    "# compute the best partition\n",
    "partition = community_louvain.best_partition(graphforgephi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update der Knoten mit der dazugehörigen community\n",
    "for i,node in enumerate(graphforgephi):\n",
    "    graphforgephi.add_node(node, partition=list(partition.values())[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speichern des Graphen, damit er in Gephi genutzt werden kann\n",
    "nx.write_gexf(graphforgephi, \"graphforgephi_08.gexf\")\n",
    "print('save done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
