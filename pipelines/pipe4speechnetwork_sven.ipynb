{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate semantic similarity networks of all speeches\n",
    "\n",
    "This is a pipeline to create the semantic similarity network of all speeches.\n",
    "\n",
    "\n",
    "#### 1. Import the basic stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/speeches_preprocessed.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reden = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step of data cleaning can be skipped!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean party labels\n",
    "for rede in reden:\n",
    "    rede['party']=rede['party'].replace(u'\\xa0', u' ')\n",
    "    if rede['party']=='Bündnis 90/Die Grünen':\n",
    "        rede['party']='BÜNDNIS 90/DIE GRÜNEN'\n",
    "    if rede['party']=='Fraktionslos':\n",
    "        rede['party']='fraktionslos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Data selection\n",
    "\n",
    "Now select, if desired, a subset of speeches by party, parlamentarians, date, etc. Note that it does not work for the 'text'-field, but 'text_lem' should work.\n",
    "\n",
    "To be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(reden[0]['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_for(what, search_terms, speeches):\n",
    "    filtered_speeches = []\n",
    "    for speech in speeches:\n",
    "        if ( speech[what] in set(search_terms) ):\n",
    "            filtered_speeches.append(speech)\n",
    "    filtered_speeches.sort(key = lambda x:x['date']) \n",
    "    return filtered_speeches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "members = ['Hansjörg Durz','Birke Bull-Bischoff','Stefan Kaufmann','Ernst Dieter Rossmann','Götz Frömming','Katja Suding','Kai Gehring','Tankred Schipanski','Saskia Esken','Oliver Kaczmarek','Nicola Beer','Anke Domscheit-Berg','Tabea Rößner','Manuel Höferlin','Sven Lehmann','Karamba Diaby','Susann Rüthrich','Katarina Barley','Sylvia Pantel','Johannes Huber','Katrin Werner','Grigorios Aggelidis','Katja Dörner','Martin Reichardt','Nadine Schön','Nicole Höchst','Stefan Schwartze','Norbert Müller','Uwe Schulz','Maik Beermann','Josephine Ortleb','Cornelia Möhring','Ulle Schauws','Silke Launert','Wiebke Esdar','Gülistan Yüksel','Matthias Seestern-Pauly','Marcus Weinberg','Martin Patzelt','Dagmar Schmidt','Anna Christmann','Uwe Kamann','Silvia Breher','Nicole Bauer','Leni Breymaier','Katrin Helling-Plahr','Annalena Baerbock','Petra Sitte','Mariana Iris Harder-Kühnel','Katja Mast','Roman Müller-Böhm','Doris Achelwilm','Yvonne Magwas','Sönke Rix','Ronja Kemmer','Margit Stumpp','Manja Schüle','Jens Brandenburg','Nicole Gohlke','Katrin Staffler','Beate Walter-Rosenheimer','Bettina Margarethe Wiesmann','Ulrike Bahr','Franziska Giffey','Anja Karliczek','Michaela Noll','Yasmin Fahimi','Melanie Bernstein','Stephan Albani','Marja-Liisa Völlers','Thomas Sattelberger','Dietlind Tiemann','René Röspel','Albert Rupprecht','Michael Espendiller','Joana Cotar','Mario Brandenburg','Volker Münz','Astrid Mannes','Ekin Deligöz','Stefan Sauer','Svenja Stadler','Swen Schulz','Kerstin Radomski','Johannes Steiniger','Caren Marks','Andreas Steier','Dieter Janecek','Sybille Benning','Thomas Rachel','Dorothee Bär','Frank Pasemann','Lars Klingbeil','Ingrid Pahlmann','Markus Paschke','Elvan Korkmaz-Emre','Charlotte Schneidewind-Hartnagel']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reden_selection = filter_for('name', members, reden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reden = reden_selection\n",
    "len(reden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reden[5102]['text_lem']))\n",
    "print(reden[5102]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build corpus and count word frequencies per party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24666/24666 [00:02<00:00, 8914.12it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "allwords = []\n",
    "\n",
    "minlength = -20;\n",
    "\n",
    "# prepare for per party counts\n",
    "allwordsperparty = {\n",
    "    'SPD':[],\n",
    "    'FDP':[],\n",
    "    'CDU/CSU':[],\n",
    "    'DIE LINKE':[],\n",
    "    'BÜNDNIS 90/DIE GRÜNEN':[],\n",
    "    'AfD':[],\n",
    "    'fraktionslos':[],\n",
    "    'Bremen':[]   \n",
    "}\n",
    "        \n",
    "#consider = ['PROPN']\n",
    "#consider = ['ADJ']\n",
    "consider = ['NOUN']\n",
    "for rede in tqdm.tqdm(reden):\n",
    "    if(len(rede['text_lem']) > minlength):\n",
    "    #rel_lemmata = [ ele for ex,ele in enumerate(rede['text_lem']) if (rede['text_pos'][ex] in consider and len(ele) > 7 and len(ele) < 16)]\n",
    "        rel_lemmata = [ ele for ex,ele in enumerate(rede['text_lem']) if rede['text_pos'][ex] in consider ]\n",
    "        allwords.extend(rel_lemmata)\n",
    "        allwordsperparty[rede['party']].extend(rel_lemmata)\n",
    "        corpus.append( \" \".join(rel_lemmata ))\n",
    "   \n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Semantic Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer_selection = CountVectorizer(vocabulary = features02, decode_error='ignore' , lowercase=False, ngram_range=(1, 1))\n",
    "vectorizer = CountVectorizer(decode_error='ignore' , lowercase=False , max_df=0.8, min_df=2, ngram_range=(1, 1))\n",
    "mm = vectorizer.fit_transform(corpus)\n",
    "features = vectorizer.get_feature_names()\n",
    "print(f'We now consider {len(features)} different words.')\n",
    "print(mm.get_shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that via TFIDF this is way faster!\n",
    "SemSimMat = cosine_similarity(mm)\n",
    "print(SemSimMat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24666, 60262)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer(lowercase=False,max_df=0.8, min_df=2/len(corpus))\n",
    "mm_tfidf  = vectorizer_tfidf.fit_transform(corpus)\n",
    "features = vectorizer_tfidf.get_feature_names()\n",
    "mm_tfidf.get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24666, 24666)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SemSimMat_tfidf = mm_tfidf * mm_tfidf.T\n",
    "SemSimMat_tfidf.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reden[0] \n",
    "print(SemSimMat_tfidf[10,2003])\n",
    "print(SemSimMat[10,2003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes = []\n",
    "for rx,rede in enumerate(reden):\n",
    "    node = {\n",
    "        'id' :  rede['id'],\n",
    "        'name' : rede['name'],\n",
    "        'date' : rede['date'],\n",
    "        'discussion_title' : rede['discussion_title'],\n",
    "        'party' : rede['party'],\n",
    "        'length' : len(rede['text_lem']),\n",
    "        #'text' : rede['text']\n",
    "    }\n",
    "    \n",
    "    nodes.append(node)\n",
    "    \n",
    "\n",
    "graph = {\n",
    "    'directed': False,\n",
    "    'graph': 'word_graph',\n",
    "    'links': [],\n",
    "    'nodes': nodes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0]\n",
    "#SemSimMat_tfidf.mean()\n",
    "SimMat = SemSimMat_tfidf.todense()\n",
    "vectorizer = vectorizer_tfidf\n",
    "mm = mm_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24666it [08:57, 45.92it/s]\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "\n",
    "min_weight = 0.2222\n",
    "for ix,nodeI in tqdm.tqdm(enumerate(graph['nodes'])):\n",
    "    for jx,nodeJ in enumerate(graph['nodes']):\n",
    "        if nodeI['id'] < nodeJ['id']:          \n",
    "            source = nodeI['id']\n",
    "            target = nodeJ['id']\n",
    "            weight = SimMat[ix,jx]\n",
    "            if weight > min_weight:\n",
    "                #links.append([source,target,weight])\n",
    "                link_dict = {\n",
    "                    'source':source,\n",
    "                    'target':target,\n",
    "                    'weight':weight       \n",
    "                }\n",
    "                graph['links'].append(link_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This graph has 24666 nodes and 313120 links.\n"
     ]
    }
   ],
   "source": [
    "nn = len(graph['nodes'])\n",
    "ne = len(graph['links'])\n",
    "print( f\"This graph has {nn} nodes and {ne} links.\")\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#weights=[]\n",
    "#for link in graph['links']:\n",
    "#    weights.append(link['weight'])\n",
    "#print(sum(weights))\n",
    "\n",
    "#plt.hist(weights, bins=25)\n",
    "#plt.title(\"Distribution of Weights\")\n",
    "#plt.xlabel(\"Wert\")\n",
    "#plt.ylabel(\"Häufigkeit\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's too much.\n",
    "\n",
    "#newlinks = []\n",
    "#for link in graph['links']:\n",
    "#    if link['weight'] > 0.3:\n",
    "#        newlinks.append(link)\n",
    "#graph['links'] = newlinks \n",
    "#len(graph['links'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The graph has {len(graph[\"links\"])} links.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append Information about relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24666it [02:02, 200.66it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for count,node in tqdm.tqdm(enumerate(graph['nodes'])):\n",
    "    vec_numbers = np.array(mm.getrow(count).toarray()[0])\n",
    "    #maxWX = np.argmax(vec_numbers)\n",
    "    #hilf.update({'vec_numbers': vec_numbers})\n",
    "    msw = list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(np.argmax(vec_numbers))]\n",
    "    #hilf.update({'maxTFIDF': msw})\n",
    "    node.update({'msw' : msw})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24666/24666 [00:00<00:00, 346954.75it/s]\n",
      " 33%|███▎      | 102383/313120 [00:00<00:00, 503200.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313120/313120 [00:00<00:00, 508066.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links done\n",
      "save done\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graphforgephi = nx.Graph()\n",
    "for node in tqdm.tqdm(graph['nodes']):\n",
    "    graphforgephi.add_node(node['id'],name = node['name'],date = node['date'],discussion_title = node['discussion_title'],party = node['party'], length = node['length'],msw = node['msw']);\n",
    "print('nodes done')\n",
    "for link in tqdm.tqdm(graph['links']):   \n",
    "    graphforgephi.add_edge(link['source'],link['target'],weight = link['weight'])\n",
    "print('links done')   \n",
    "nx.write_gexf(graphforgephi, \"../private/graphforgephi.gexf\")\n",
    "print('save done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
