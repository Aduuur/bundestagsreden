{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import spacy\n",
    "nlp = spacy.load('de') #load spacy model\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "import collections\n",
    "\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/seven/data/git/mpileipzig/penelope/components/\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see devStatementGraph and devPaperPipeline for development and documentation\n",
    "def StatementGraphGenerator(inParams):\n",
    "\n",
    "    stm_list = inParams['data']\n",
    "    \n",
    "    if inParams['language'] == 'en':\n",
    "        nlp = spacy.load('en_core_web_sm') #load english spacy model\n",
    "    elif inParams['language'] == 'de':\n",
    "        nlp = spacy.load('de_core_news_sm') #load german spacy model\n",
    "    elif inParams['language'] == 'fr':\n",
    "        nlp = spacy.load('fr_core_news_sm') #load french spacy model\n",
    "    elif inParams['language'] == 'es':\n",
    "        nlp = spacy.load('es_core_news_sm') #load spanish spacy model\n",
    "    elif inParams['language'] == 'pt':\n",
    "        nlp = spacy.load('pt_core_news_sm') #load portuguese spacy model\n",
    "    elif inParams['language'] == 'it':\n",
    "        nlp = spacy.load('it_core_news_sm') #load italian spacy model\n",
    "    elif inParams['language'] == 'nl':\n",
    "        nlp = spacy.load('nl_core_news_sm') #load dutch spacy model\n",
    "    elif inParams['language'] == 'el':\n",
    "        nlp = spacy.load('el_core_news_sm') #load greek spacy model\n",
    "    elif inParams['language'] == 'nb':\n",
    "        nlp = spacy.load('nb_core_news_sm') #load norwegian bokmal spacy model\n",
    "    elif inParams['language'] == 'lt':\n",
    "        nlp = spacy.load('lt_core_news_sm') #load lithuanian spacy model\n",
    "    elif inParams['language'] == 'xx':\n",
    "        nlp = spacy.load('xx_ent_wiki_sm') #load multi-language spacy model\n",
    "\n",
    "    else:\n",
    "        print('ERROR! Language not supported!')\n",
    "        \n",
    "    relevantPOS = inParams['pos']\n",
    "    ignoreLEM = inParams['ignore']\n",
    "    \n",
    "    # 1.2.2 Note: requires spacy and language model to be loaded\n",
    "    print('apply nlp pipeline')\n",
    "    for value in tqdm.tqdm(stm_list):\n",
    "        doc = nlp(value['text'])\n",
    "        value.update({'doc' : doc})\n",
    "    \n",
    "    # 1.2.3\n",
    "    # Data cleaning options\n",
    "    print('filter words')\n",
    "    #relevantPOS = ['NOUN','VERB','ADJ']\n",
    "    #ignoreLEM = ['Herr','Kollege','Dame','Frau','Präsident']\n",
    "    #ignoreLEM = []\n",
    "\n",
    "    stm_list_nlp = []\n",
    "    statements_lemma_list = []\n",
    "    #statements_pos_list = []\n",
    "    for stm in tqdm.tqdm(stm_list):\n",
    "        lemmas = []\n",
    "        pos = []\n",
    "        for token in stm['doc']:\n",
    "            if token.is_punct is not True and token.is_stop is not True and token.pos_ in relevantPOS and token.lemma_ not in ignoreLEM:\n",
    "                lemmas.append(token.lemma_)\n",
    "                pos.append(token.pos_)\n",
    "        stm.update({'lemmata' : lemmas})    \n",
    "        statements_lemma_list.append(lemmas)\n",
    "        #statements_pos_list.append(pos)\n",
    "\n",
    "    # 1.2.4\n",
    "    print('count and order lemmata')\n",
    "    flattened = [val for lemmas in statements_lemma_list for val in lemmas]\n",
    "    lemma_frequency_list = collections.Counter(flattened).most_common()\n",
    "    all_lemmata_list = [ i[0] for i in lemma_frequency_list ]\n",
    "    all_lemmata_count = [ i[1] for i in lemma_frequency_list ]\n",
    "\n",
    "    # remove single items\n",
    "    # 1.2.5\n",
    "    print('remove singles')\n",
    "    all_freq_lemmata_list = [ all_lemmata_list[i] for i,v in enumerate(all_lemmata_list) if (all_lemmata_count[i] > 1) ]\n",
    "    all_freq_lemmata_count = [ all_lemmata_count[i] for i,v in enumerate(all_lemmata_count) if (all_lemmata_count[i] > 1) ]\n",
    "        \n",
    "    all_lemmata_list = all_freq_lemmata_list\n",
    "    all_lemmata_count = all_freq_lemmata_count \n",
    "\n",
    "    # Construct word browsable dict for later use in semantic relatedness computation\n",
    "    wordscounts_dict = {}\n",
    "    for lx,lem in enumerate(all_lemmata_list):\n",
    "        wordscounts_dict.update({ lem : all_lemmata_count[lx] })\n",
    " \n",
    "    # From this file\n",
    "    stmnet_dict = {\n",
    "        'nodes' : [],\n",
    "        'links' : []\n",
    "        }    \n",
    " \n",
    "\n",
    "    for ix,stm in enumerate(stm_list):\n",
    "        if len(stm['lemmata']) > 0:\n",
    "            node_dict = {\n",
    "                'id' : ix+1\n",
    "            }\n",
    "            for ele in stm:\n",
    "                if ele != 'lemmata' and ele != 'doc':\n",
    "                    node_dict.update({ele : stm[ele]})\n",
    "            \n",
    "            mfic = [ lem for lem in all_lemmata_list if (lem in stm['lemmata'] )]\n",
    "            #freqI = collections.Counter(stmI).most_common()\n",
    "            #lemmataI = [ i[0] for i in freqI ]\n",
    "            if len(mfic)>0:\n",
    "                node_dict.update({'mfic' : mfic[0]})\n",
    "            else:\n",
    "                node_dict.update({'mfic' : ' '})\n",
    "    \n",
    "            stmnet_dict['nodes'].append(node_dict)\n",
    "    \n",
    "    \n",
    "    #stmnet_dict['graph']['edges'] = []\n",
    "    for nodeI in tqdm.tqdm(stmnet_dict['nodes']):\n",
    "        stmI = stm_list[ nodeI['id']-1 ]['lemmata']\n",
    "        for nodeJ in stmnet_dict['nodes']:     \n",
    "            if nodeI['id'] < nodeJ['id']:\n",
    "                stmJ = stm_list[ nodeJ['id']-1 ]['lemmata']\n",
    "                overlap = [ lem for lem in set(stmI) if (lem in set(stmJ))]  # set overlap\n",
    "                if (len(overlap) > 0):\n",
    "                    edge_dict = {\n",
    "                        'source' : nodeI['id'],\n",
    "                        'target' : nodeJ['id'],\n",
    "                        'weight' : len(overlap)\n",
    "                        }\n",
    "                    stmnet_dict['links'].append(edge_dict)\n",
    "    \n",
    "    return stmnet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_query = \"Digitalisierung\"\n",
    "#search_query = \"Klima\"\n",
    "#search_query = \"Sozialismus\"\n",
    "#search_query = \"Kräfte\"\n",
    "\n",
    "search_query = \"erneuerbar\"\n",
    "search_query2 = \"wirtschaftlich\"\n",
    "\n",
    "json = {'search_query':search_query, \n",
    "        'dataset_name':'deu', \n",
    "        'start_date':'2016-1-1', \n",
    "        'end_date':'2021-1-1'}\n",
    "result = requests.post('https://penelope.vub.be/parliament-data/get-speeches', json=json)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = []\n",
    "for speech in result.json()[\"speeches\"]:\n",
    "    speeches.append(speech)\n",
    "    #parties.append(speech['party'])\n",
    "    #print(speech['date'])\n",
    "\n",
    "# code to sort list on date \n",
    "speeches.sort(key = lambda x:x['date'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {'search_query':search_query2, \n",
    "        'dataset_name':'deu', \n",
    "        'start_date':'2016-1-1', \n",
    "        'end_date':'2021-1-1'}\n",
    "result = requests.post('https://penelope.vub.be/parliament-data/get-speeches', json=json)\n",
    "\n",
    "speeches2 = []\n",
    "for speech in result.json()[\"speeches\"]:\n",
    "    speeches2.append(speech)\n",
    "\n",
    "# code to sort list on date \n",
    "speeches2.sort(key = lambda x:x['date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95 speeches containing _erneuerbar_ and _wirtschaftlich_\n"
     ]
    }
   ],
   "source": [
    "jointspeeches=[]\n",
    "for speech in speeches:\n",
    "    if search_query in speech['text'] and search_query2 in speech['text']:\n",
    "        jointspeeches.append(speech)\n",
    "ljoint = len(jointspeeches)\n",
    "print(f\"There are {ljoint} speeches containing _{search_query}_ and _{search_query2}_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:07<00:00, 12.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'Andreas Jung',\n",
       "  'party': 'CDU/CSU',\n",
       "  'text': 'Wenn wir aus den fossilen Energien aussteigen, müssen wir Alternativen schaffen, und die erneuerbaren Energien sind der richtige Weg, um Deutschland wirtschaftlich nachhaltig aufzustellen.'},\n",
       " {'name': 'Peter Altmaier',\n",
       "  'party': 'CDU/CSU',\n",
       "  'text': 'Der Ausbau der erneuerbaren Energien ist heute zu weitaus geringeren Kosten möglich als in früheren Jahren, weil wir ihn marktwirtschaftlicher gemacht haben.'},\n",
       " {'name': 'Carsten Müller',\n",
       "  'party': 'CDU/CSU',\n",
       "  'text': 'Die Grünen greifen das Thema der sogenannten Bürgerenergiegenossenschaften auf und wenden sich explizit gegen marktwirtschaftliche Ausschreibungen im Bereich der erneuerbaren Energien.'},\n",
       " {'name': 'Till Mansmann',\n",
       "  'party': 'FDP',\n",
       "  'text': '– Herr Minister, für den wirtschaftlichen Aufbau vieler Entwicklungsländer mit sonnenreichem trockenem Klima wäre die Möglichkeit der Vermarktung des dort verfügbaren enormen Potenzials an erneuerbaren Energien sehr wichtig.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stm_list=[]\n",
    "for speech in tqdm.tqdm(jointspeeches):\n",
    "    doc = nlp(speech[\"text\"])\n",
    "    for sent in doc.sents:\n",
    "        if search_query in sent.text and search_query2 in sent.text:\n",
    "            #print(sent)\n",
    "            stm_dict = {\n",
    "                'name'  : speech[\"name\"],\n",
    "                'party' : speech[\"party\"],\n",
    "                'text'  : sent.text\n",
    "            }\n",
    "            stm_list.append(stm_dict)\n",
    "stm_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:07<00:00, 12.57it/s]\n"
     ]
    }
   ],
   "source": [
    "stm_list=[]\n",
    "for speech in tqdm.tqdm(jointspeeches):\n",
    "    doc = nlp(speech[\"text\"])\n",
    "    if search_query in speech[\"text\"] and search_query2 in speech[\"text\"]:\n",
    "        #print(sent)\n",
    "        stm_dict = {\n",
    "            'name'  : speech[\"name\"],\n",
    "            'party' : speech[\"party\"],\n",
    "            'text'  : speech[\"text\"]\n",
    "        }\n",
    "        stm_list.append(stm_dict)\n",
    "#stm_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = []\n",
    "\n",
    "dates = []\n",
    "for speech in speeches:\n",
    "    parties.append(speech['party'])\n",
    "    dates.append(speech['date'])\n",
    "    #print(speech['date'],speech['party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Create a column from the datetime variable\n",
    "df['datetime'] = dates\n",
    "# Convert that column into a datetime datatype\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "# Set the datetime column as the index\n",
    "df.index = df['datetime'] \n",
    "# Create a column from the numeric score variable\n",
    "df['party'] = parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-10-24</th>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>CDU/CSU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21</th>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>CDU/CSU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21</th>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>CDU/CSU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21</th>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>CDU/CSU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22</th>\n",
       "      <td>2017-11-22</td>\n",
       "      <td>CDU/CSU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>2019-11-15</td>\n",
       "      <td>DIE LINKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>2019-11-15</td>\n",
       "      <td>CDU/CSU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>2019-11-15</td>\n",
       "      <td>BÜNDNIS 90/DIE GRÜNEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>2019-11-15</td>\n",
       "      <td>CDU/CSU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>2019-11-15</td>\n",
       "      <td>BÜNDNIS 90/DIE GRÜNEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1040 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime                  party\n",
       "datetime                                    \n",
       "2017-10-24 2017-10-24                CDU/CSU\n",
       "2017-11-21 2017-11-21                CDU/CSU\n",
       "2017-11-21 2017-11-21                CDU/CSU\n",
       "2017-11-21 2017-11-21                CDU/CSU\n",
       "2017-11-22 2017-11-22                CDU/CSU\n",
       "...               ...                    ...\n",
       "2019-11-15 2019-11-15              DIE LINKE\n",
       "2019-11-15 2019-11-15                CDU/CSU\n",
       "2019-11-15 2019-11-15  BÜNDNIS 90/DIE GRÜNEN\n",
       "2019-11-15 2019-11-15                CDU/CSU\n",
       "2019-11-15 2019-11-15  BÜNDNIS 90/DIE GRÜNEN\n",
       "\n",
       "[1040 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data frame by month and item and extract a number of stats from each group\n",
    "total = df.resample('M').agg({'party':'count'})\n",
    "#total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            party\n",
       "datetime         \n",
       "2017-10-31    1.0\n",
       "2017-11-30    1.0\n",
       "2017-12-31    1.0\n",
       "2018-01-31    1.0\n",
       "2018-02-28    1.0\n",
       "2018-03-31    1.0\n",
       "2018-04-30    1.0\n",
       "2018-05-31    1.0\n",
       "2018-06-30    1.0\n",
       "2018-07-31    1.0\n",
       "2018-08-31    NaN\n",
       "2018-09-30    1.0\n",
       "2018-10-31    1.0\n",
       "2018-11-30    1.0\n",
       "2018-12-31    1.0\n",
       "2019-01-31    1.0\n",
       "2019-02-28    1.0\n",
       "2019-03-31    1.0\n",
       "2019-04-30    1.0\n",
       "2019-05-31    1.0\n",
       "2019-06-30    1.0\n",
       "2019-07-31    NaN\n",
       "2019-08-31    NaN\n",
       "2019-09-30    1.0\n",
       "2019-10-31    1.0\n",
       "2019-11-30    1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CDU/CSU': 400,\n",
       " 'SPD': 205,\n",
       " 'FDP': 167,\n",
       " 'BÜNDNIS 90/DIE GRÜNEN': 92,\n",
       " 'AfD': 87,\n",
       " 'DIE LINKE': 58,\n",
       " 'BÜNDNIS\\xa090/DIE GRÜNEN': 23,\n",
       " 'fraktionslos': 8}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(speeches))\n",
    "partycounts = dict(Counter(parties).most_common())\n",
    "partycounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2018-02-22',\n",
       " 'discussion_title': 'Tagesordnungspunkt 7 2018-02-22',\n",
       " 'id': 'ID191404500',\n",
       " 'name': 'Anja Weisgerber',\n",
       " 'party': 'CDU/CSU',\n",
       " 'period': '19',\n",
       " 'text': 'Sehr geehrte Frau Präsidentin! Werte Kolleginnen und Kollegen! Klimaschutz ist eine große Herausforderung. Er ist wichtig, für manche Inseln im Pazifik ist er eine Überlebensfrage. Aber auch bei uns in Deutschland sind die Auswirkungen des Klimawandels bereits spürbar. Die Entwicklung von Innovationen im Bereich Klimaschutz ist eine Riesenchance für die Wirtschaft. Wir müssen am Ball bleiben. Wir müssen den Wandel gestalten. Wir dürfen ihn nicht verschlafen. Ich war gestern im Umweltministerium, wo der Deutsche Innovationspreis für Klima und Umwelt an mutige Unternehmer verliehen wurde. Auch mir machte diese Veranstaltung Mut, weil ich gesehen habe, mit welcher Kraft und mit welcher Überzeugung die Unternehmer bei der Sache sind. Es wurden tolle Projekte vorgestellt. Zum Beispiel kann Papier aus Gras hergestellt werden, um die Ressource Holz als wertvollen CO 2 -Speicher zu schonen. Ein weiteres Projekt war eine energiesparende Maschine zur Stoffherstellung, die gleichzeitig spinnt und strickt. Sie sehen: Es gibt eine Menge Innovationen durch mutige mittelständische Unternehmer, wodurch Arbeitsplätze geschaffen und auch erhalten werden können. Im ausverhandelten Koalitionsvertrag haben wir ganz klar festgehalten, dass wir uns zu unseren nationalen, europäischen und internationalen Klimazielen bekennen, und zwar in allen Sektoren. Dafür habe ich mich als Klimapolitikerin persönlich eingesetzt. Damit zeigen wir: Wir nehmen unsere klimapolitische Verantwortung wahr, in Deutschland und in der Welt. Für uns ist aber auch wichtig, wie wir dieses Ziel erreichen. Das wollen wir durch Anreize statt Zwang und durch Technologieoffenheit bewerkstelligen. Wir wollen auch sicherstellen, dass unsere Wirtschaft als Basis unseres Wohlstands international wettbewerbsfähig bleibt. Außerdem soll die Energieversorgungssicherheit weiterhin gewährleistet werden. Es liegt eine Studie der deutschen Industrie vor, die sich damit befasst hat, wie wir das langfristige Klimaziel bezogen auf 2050 erreichen wollen. Die Studie besagt: Das wird schwierig, aber es kann funktionieren. Und auch die Wirtschaft sagt: Die Wirtschaft kann Wandel. Sie hat es bei der Industrialisierung bewiesen. Jetzt nimmt sie die Herausforderungen der Digitalisierung an, Stichwort Industrie 4.0. Die Wirtschaft kann auch Klimaschutz, wenn die Politik verlässliche Rahmenbedingungen schafft und Anreize zum Beispiel für effiziente Umwelttechnologien setzt. Und genau das machen wir, meine Damen und Herren. Wenn Sie, die Antragsteller, deren Vorlagen wir heute beraten, den ausgehandelten Koalitionsvertrag aufmerksam lesen, dann werden Sie feststellen, dass viele Ihrer Punkte bereits enthalten sind, zum Beispiel die Forderung nach einer konsequenten Energieeffizienzpolitik. Es gibt bereits den Nationalen Aktionsplan Energieeffizienz, den wir weiterentwickeln werden. Außerdem wollen wir eine ambitionierte, sektorübergreifende Energieeffizienzstrategie unter dem Leitprinzip „Efficiency first“ installieren. Wir haben uns das ehrgeizige Ziel gesetzt, eine Halbierung des Energieverbrauchs bis 2050 zu schaffen. Im Koalitionsvertrag ist vorgesehen, bis 2030 einen Anteil an erneuerbaren Energien von 65 Prozent zu erreichen. Ebenso haben wir verankert – ich muss ganz ehrlich sagen: endlich –, die energetische Gebäudesanierung steuerlich zu fördern. In vielen Reden an diesem Rednerpult habe ich gefordert, dass wir dieses Klimaschutzinstrument nutzen. Meine Damen und Herren, ich setze darauf, dass nun auch die Grünen in den Bundesländern für dieses Instrument werben; denn in den Jamaika-Sondierungen war die Einführung dieses Instruments Konsens. Damit bin ich beim Thema: Wie sollen diese Instrumente ausgestaltet sein? Wie soll dieses Steuerungsinstrument ausgestaltet sein? Wir setzen auf Anreize statt auf Zwang. Unser Vorschlag sieht ein Wahlrecht des Antragstellers vor: Zuschussförderung oder Reduzierung des zu versteuernden Einkommens. Damit erreichen wir einen möglichst großen Personenkreis. Daneben wollen wir das CO 2 -Gebäudesanierungsprogramm fortführen und damit den Austausch alter, ineffizienter Heizungen fördern. Ferner wollen wir eine Kommission einsetzen, die auf Basis und in Vernetzung mit dem laufenden Prozess zur Umsetzung des Klimaschutzplans bis Ende des Jahres Maßnahmen vorschlagen soll, wie die Lücke zur Erreichung des Klimaziels 2020 so schnell wie möglich reduziert werden kann und im Energiesektor das 2030-Ziel erreicht werden kann. Dazu zählt auch die schrittweise Reduzierung und Beendigung der Kohleverstromung. Das Abschlussdatum für die Kohleverstromung soll ebenfalls durch diese Kommission festgelegt werden. Hört! Hört! Meine Damen und Herren, das geht sogar über das hinaus, was am Ende der Jamaika-Sondierungen ausverhandelt war. In den Jamaika-Verhandlungen lag der Fokus – Herr Dr. Köhler hat es bereits erwähnt – auf dem überhasteten, schnellen Stilllegen, auf der Reduzierung der Kapazität der Kohleverstromung um 7 Gigawatt vor 2020, und zwar ohne Rücksicht auf die Arbeitsplätze. Auch wir wollen die schrittweise Reduzierung der Kohleverstromung; aber wir wollen dabei die Energieversorgung weiterhin sicherstellen. Wir wollen, dass Energie für Verbraucher und Wirtschaft bezahlbar bleibt. Das ist ganz wichtig. Ich habe das Gefühl, das wird von den Grünen öfter ausgeblendet. Außerdem wollen wir nicht, dass es in den betroffenen Regionen zu Strukturbrüchen kommt. Sie sehen: Wir reden nicht nur, sondern wir handeln auch, aber durchdacht und mit Weitsicht. Zur Globalen Allianz für den Kohleausstieg, die in einem Antrag erwähnt wird: Gefordert wird, dass Deutschland dieser Allianz beitritt. Ich habe mir die Mühe gemacht, mir den Energiemix der teilnehmenden Staaten anzusehen. Man muss sich da ehrlich machen. Wie ist das in Frankreich? Dort stammen nur 3 bis 4 Prozent der Stromproduktion aus der Kohle, und Frankreich will von dem Ziel abrücken, den Strom aus Kernenergie bis 2025 auf 50 Prozent zu reduzieren. Kanada investiert mehr als 25 Milliarden in Atomreaktoren, damit sie für weitere 25 bis 30 Jahre laufen. Großbritannien will aus der Kohleverstromung aussteigen, gleichzeitig aber den Anteil des Stroms aus Kernenergie deutlich steigern. Wir wollen beides: Wir wollen die Kohleverstromung reduzieren und den Ausstieg aus der Kernenergienutzung weiter gestalten. Keiner behauptet, dass das leicht ist. Dennoch gehen wir diesen Weg voller Überzeugung für unsere Kinder, für unsere Enkel und für die Menschen, die darauf setzen, dass die Politik die Weichen richtig stellt und durch Innovationen im Umweltbereich Arbeitsplätze erhalten und geschaffen werden. Vielen Dank. Vielen Dank, Dr. Weisgerber. – Nächste Rednerin: Bundesministerin Dr. Barbara Hendricks für die Bundesregierung. '}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 851/851 [00:57<00:00, 14.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stm_list=[]\n",
    "for speech in tqdm.tqdm(speeches):\n",
    "    doc = nlp(speech[\"text\"])\n",
    "    for sent in doc.sents:\n",
    "        if search_query in sent.text:\n",
    "            #print(sent)\n",
    "            stm_dict = {\n",
    "                'name'  : speech[\"name\"],\n",
    "                'party' : speech[\"party\"],\n",
    "                'text'  : sent.text\n",
    "            }\n",
    "            stm_list.append(stm_dict)\n",
    "    \n",
    "len(stm_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ele in stm_list:\n",
    "#    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/95 [00:00<00:05, 16.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply nlp pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:07<00:00, 12.80it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 2556.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter words\n",
      "count and order lemmata\n",
      "remove singles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:01<00:00, 58.12it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inParams = {\n",
    "    'data': stm_list,\n",
    "    'language' : \"de\",\n",
    "    #'pos' : ['NOUN','VERB','ADJ'],\n",
    "    'pos' : ['NOUN'],\n",
    "    'ignore' : [search_query,search_query2]\n",
    "}\n",
    "\n",
    "graph = StatementGraphGenerator(inParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4465 95\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(graph['links']),\n",
    "len(graph['nodes']))\n",
    "#print(graph['nodes'])\n",
    "#graph['links']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 95\n"
     ]
    }
   ],
   "source": [
    "newlinks = []\n",
    "for link in graph['links']:\n",
    "    if link['weight'] > 25:\n",
    "        newlinks.append(link)\n",
    "graph['links'] = newlinks    \n",
    "print(len(graph['links']),\n",
    "len(graph['nodes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {'data':graph, \n",
    "        'nodecoloring':'party', \n",
    "        'nodelabel': 'text', \n",
    "        \"darkmode\": False,\n",
    "        \"edgevisibility\": True,\n",
    "        \"particles\": False\n",
    "       }\n",
    "result = requests.post('https://penelope.vub.be/network-components/visualiser', json=json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"./satzgraph.html\", \"w\") as f:\n",
    "    f.write(result.json()['graph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check: Das funktioniert irgendwie nicht!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Doc is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-7df2a34d65aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"relevant_pos\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"NOUN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m        }\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://penelope.vub.be/network-components/statementgraphgenerator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         )\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_cookies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         )\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_body\u001b[0;34m(self, data, files, json)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;31m# provides this natively, but Python 3 gives a Unicode string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'application/json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Doc is not JSON serializable"
     ]
    }
   ],
   "source": [
    "#stm_list =[{'text': 'Das war ein Satz.'},\n",
    "#           {'text': 'Der Satz des Tages ist das.'} \n",
    "#]\n",
    "\n",
    "json = {'data':stm_list, \n",
    "        'language':'de', \n",
    "        'ignore': ['Digitalisierung'], \n",
    "        #\"relevant_pos\": [\"VERB\",\"NOUN\",\"ADJ\"]\n",
    "        \"relevant_pos\": [\"NOUN\"]\n",
    "       }\n",
    "result = requests.post('https://penelope.vub.be/network-components/statementgraphgenerator', json=json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "[{'id': 1, 'mfic': '', 'text': 'Das war ein Satz.'}, {'id': 2, 'mfic': '', 'text': 'Der Satz des Tages ist das.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = result.json()\n",
    "print(len(graph['links']),\n",
    "len(graph['nodes']))\n",
    "print(graph['nodes'])\n",
    "graph['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
