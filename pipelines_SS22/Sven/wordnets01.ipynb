{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "import re\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('de') #load spacy model\n",
    "\n",
    "# for WordClouds \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "with open(\"../../data/speeches_20_withoutcomments.jsonl\",'r',encoding = \"utf8\") as fp:\n",
    "    data = list(fp)\n",
    "speeches = []\n",
    "for line in data:\n",
    "    speeches.append(json.loads(line))\n",
    "\n",
    "# clean party labels\n",
    "for rede in speeches:\n",
    "    rede['party']=rede['party'].replace(u'\\xa0', u' ')\n",
    "    if rede['party']=='Bündnis 90/Die Grünen':\n",
    "        rede['party']='BÜNDNIS 90/DIE GRÜNEN'\n",
    "    if rede['party']=='Fraktionslos':\n",
    "        rede['party']='fraktionslos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# last changed: 02/06/2022\n",
    "def filter_for(what, search_terms, speeches):\n",
    "    filtered_speeches = []\n",
    "    if what == 'text':\n",
    "        search_terms_low = []\n",
    "        for st in search_terms:\n",
    "            search_terms_low.append(st.lower())\n",
    "        for speech in speeches:\n",
    "            match = [st in speech[what].lower() for st in search_terms_low]\n",
    "            #if all(st == True for st in match):\n",
    "            if any(st in speech[what] for st in search_terms):\n",
    "                #print(match)\n",
    "            #if ( search_terms in speech[what] ):\n",
    "                filtered_speeches.append(speech)\n",
    "    else:\n",
    "        for speech in speeches:\n",
    "            if ( speech[what] in set(search_terms) ):\n",
    "                filtered_speeches.append(speech)\n",
    "        \n",
    "    filtered_speeches.sort(key = lambda x:x['date'])   \n",
    "    return filtered_speeches\n",
    "\n",
    "\n",
    "#focal_terms = ['Digitalisierung','Zusammenhalt','Demokratie']\n",
    "focal_terms = ['Digitalisierung']\n",
    "focal_terms = ['Plattform','Demokratie']\n",
    "#focal_terms = ['extrem','Plattform']\n",
    "#focal_terms = ['plattform','demokratie']\n",
    "#focal_terms = ['Plattform','Meinung']\n",
    "#focal_terms = ['Netzwerk','Meinung']\n",
    "#focal_terms = ['Volk ','Volk.','Volk!','Volk?']\n",
    "subset = filter_for('text', focal_terms, speeches)\n",
    "#subset = filter_for('party', ['AfD'], subset)\n",
    "#subset = filter_for('party', ['BÜNDNIS 90/DIE GRÜNEN'], subset)\n",
    "\n",
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 358/358 [00:31<00:00, 11.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "#focal_terms_sen = ['Netzwerk','sozial']\n",
    "#focal_terms_sen = ['Russland']\n",
    "focal_terms_sen = focal_terms #['Klimawandel']\n",
    "window = 10\n",
    "for rede in tqdm.tqdm(subset):\n",
    "    doc = nlp(rede[\"text\"])\n",
    "    sents = doc.sents\n",
    "#    mysents = []\n",
    "#    for sx,sent in enumerate(sents):\n",
    "#        mysents.append(sent)\n",
    "    for sx,sent in enumerate(sents):\n",
    "        #print(sent.text)\n",
    "        if all(ft.lower() in sent.text.lower() for ft in focal_terms_sen):\n",
    "            sentences.append(sent)\n",
    "#            if sx > 0 and sx < len(mysents)-1:\n",
    "#                sentences.append(mysents[sx-1])\n",
    "#                sentences.append(mysents[sx+1])\n",
    "        \n",
    "        \n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevantPOS = ['NOUN','ADJ','PROPN']\n",
    "sentencesNN = []\n",
    "words = []\n",
    "for sen in sentences:\n",
    "    lem = []\n",
    "    for token in sen:\n",
    "        if token.pos_ in relevantPOS:\n",
    "            lem.append(token.lemma_)\n",
    "    sentencesNN.append(lem)\n",
    "    words.extend(lem)\n",
    "#sentencesNN\n",
    "len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zähler = collections.Counter(words).most_common()\n",
    "#print(zähler)\n",
    "\n",
    "words_clean = []\n",
    "#print(focal_terms_sen)\n",
    "for word in words:\n",
    "    #if focal_terms_sen[0] in word or focal_terms[0] in word or 'Herr' in word or 'Dame' in word or 'Kollege' in word or 'Kollegin' in word or \"/\" in word:\n",
    "    if 'Herr' in word or 'Dame' in word or 'Kollege' in word or 'Kollegin' in word or \"/\" in word:\n",
    "        pass\n",
    "    else:\n",
    "        words_clean.append(word)\n",
    "    \n",
    "#print(len(words_clean))\n",
    "zähler = collections.Counter(words_clean).most_common()\n",
    "#print(zähler)\n",
    "\n",
    "num_words = 1000\n",
    "count = 0\n",
    "new_words = []\n",
    "for word in zähler:\n",
    "    #print(word[0])\n",
    "    if count < num_words:\n",
    "        new_words.append(word[0])\n",
    "#        if w1 == focal_terms_sen[0]:\n",
    "#            wxFocus = count\n",
    "    count = count + 1    \n",
    "\n",
    "# print(new_words[wxFocus])\n",
    "len(new_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:00, 9534.57it/s]\n"
     ]
    }
   ],
   "source": [
    "n = len(new_words) \n",
    "A = np.ndarray(shape=(n,n), dtype=float)\n",
    "\n",
    "for wx1,w1 in tqdm.tqdm(enumerate(new_words)):\n",
    "    #print(zähler[wx1][0])\n",
    "\n",
    "        \n",
    "    for wx2,w2 in enumerate(new_words):\n",
    "        if(wx2 > wx1):\n",
    "            A[wx1][wx2] = 0\n",
    "            for sen in sentencesNN:\n",
    "                if w1 in sen and w2 in sen:\n",
    "                    \n",
    "                    A[wx1][wx2] = A[wx1][wx2] + 1\n",
    "                    \n",
    "        \n",
    "            normcount = zähler[wx1][1]*zähler[wx2][1]\n",
    "            #if(normcount < 2):\n",
    "            #    print(normcount)\n",
    "            A[wx1][wx2] = A[wx1][wx2]/normcount\n",
    "            A[wx2][wx1] = A[wx1][wx2]            \n",
    "\n",
    "#plt.imshow(A)\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wxFocus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2ac333389028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Numpy Array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwxFocus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Getting indices of N = 3 maximum values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumWords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wxFocus' is not defined"
     ]
    }
   ],
   "source": [
    "numWords = 70\n",
    "final_words = []\n",
    "\n",
    "# Numpy Array\n",
    "scores = A[wxFocus]\n",
    "# Getting indices of N = 3 maximum values\n",
    "x = np.argsort(scores)[::-1][:numWords]\n",
    "#print(\"Indices:\",x)\n",
    "# Getting N maximum values\n",
    "print(\"Values:\",scores[x])\n",
    "\n",
    "for ele in x:\n",
    "    final_words.append( new_words[ele] )\n",
    "\n",
    "final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231 42\n"
     ]
    }
   ],
   "source": [
    "final_words = new_words\n",
    "nodes=[]\n",
    "curid=1\n",
    "for word in final_words:\n",
    "    node = {\n",
    "    'id' :  curid,\n",
    "    'name' : word\n",
    "    }\n",
    "    nodes.append(node)\n",
    "    curid=curid+1\n",
    "    \n",
    "graph = {\n",
    "    'directed': False,\n",
    "    'graph': 'word_graph',\n",
    "    'links': [],\n",
    "    'nodes': nodes\n",
    "}\n",
    "\n",
    "links = []\n",
    "linkedwords =[]\n",
    "linkedids =[]\n",
    "lx = 0;\n",
    "for wx1,w1 in enumerate(final_words):\n",
    "    #print(wx1)\n",
    "    for wx2,w2 in enumerate(final_words):\n",
    "        if(wx2 > wx1) and A[wx1][wx2]>0.001:\n",
    "            link_dict = {\n",
    "                    'source':wx1+1,\n",
    "                    'target':wx2+1,\n",
    "                    'weight': A[wx1][wx2]      \n",
    "                    }\n",
    "            graph['links'].append(link_dict)\n",
    "                      \n",
    "                    #for link in links:\n",
    "\n",
    "\n",
    "#graph['links']=links\n",
    "#print(zähler)\n",
    "print(len(graph['links']),\n",
    "len(graph['nodes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for link in graph['links']:\n",
    "#    if link['weight']>1:\n",
    "        #print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graphforgephi = nx.Graph()\n",
    "for node in tqdm.tqdm(graph['nodes']):\n",
    "    graphforgephi.add_node(node['id'],name = node['name']);\n",
    "print('nodes done')\n",
    "for link in tqdm.tqdm(graph['links']):\n",
    "    #weight = all((' '.join([w1['name'],w2['name']]) in linkedwords)\n",
    "    #print(weight)         \n",
    "    graphforgephi.add_edge(link['source'],link['target'],weight=link['weight'])\n",
    "print('links done')   \n",
    "nx.write_gexf(graphforgephi, \"graphforgephi.gexf\")\n",
    "print('save done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f1ab394b7e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'networkdata.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('networkdata.json', 'w') as fp:\n",
    "    json.dump(graph, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            for sen in sentencesNN:\n",
    "                if w1['name'] in sen and w2['name'] in sen:\n",
    "                    weight = len([ele for ele in linkedwords if ele == (' '.join([w1['name'],w2['name']]))])\n",
    "                    #    links[]\n",
    "                    #else:\n",
    "                    #print(weight)\n",
    "                    linkedwords.append(' '.join([w1['name'],w2['name']]))\n",
    "                    #linkedids[ w1['id'] ] , w2['id'] )\n",
    "                    link_dict = {\n",
    "                    'source':w1['id'],\n",
    "                    'target':w2['id'],\n",
    "                    'sourceWD':w1['name'],\n",
    "                    'targetWD':w2['name'],\n",
    "                    'weight': weight+1      \n",
    "                    }\n",
    "                    #print(link_dict)\n",
    "                    links.append(link_dict)\n",
    "                    graph['links'].append(link_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
