{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses jupyter Notebook dient dem analysieren und visualisieren der verwendeten Semantik von Bundestagsparteien im Plenarsaal.\n",
    "Seine Funktion ist es alle gesprochenen Wörter herauszufiltern, die ausschließlich von einer Partei und nicht den anderen Parteien verwendet wurden, außerdem noch die von zwei Parteien geminsam und wiederum nicht von den anderen Parteien verwendet wurden. Das Ziel ist es, Parteien aufgrund ihres Jargons und semantischer Charakteristika zu unterscheiden, Zusammenhänge zu erkennen und eventuell einordnen zu können.\n",
    "Als Datensatz dienen die digitalisierten Reden in den Plenarprotokollen des deutschen Bundestages der 19. Wahlperiode. Mit den analysierten Materialien lassen sich am Ende Wortgraphen erzeugen. Betrachtet werden die 6 Parteien, die über die 5 Prozent Klausel in den deutschen Bundestag gewählt wurden und ihn maßgeblich gestalten. Außen vor bleiben fraktionslose Kandidaten oder gesonderte Abgeordnete aus Bremen im Datensatz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module und Bibliotheken einbinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgewählte Module und Bibliotheken werden bereitgestellt, die man später in den geschriebenen Methoden aufrufen kann.\n",
    "Zuerst wird das json-Modul importiert um json-Dateien (JavaScript Object Notations; z.B. die Bundestagsredenliste) nutzen zu können.\n",
    "Die Matplotlib ist ein Visualisierungsmodul. Zu ihr gehört auch das pyplot-Modul, welches gerangezogen wird um Darstellung ähnlich der bekannten Matlab-Platform zu ermöglichen. Als Pseudonym wird dazu fortlaufend alternativ plt verwenden.\n",
    "Die Wordcloud bildet die Häufigkeit vom Vorkommen von Wörtern mittels einer Grafik ab.\n",
    "PIL (Python Imaging Library) ist eine Bibliothek zum arbeiten mit verschiedenen Bilddateiformaten.\n",
    "Um später die Grafik zu erstellen fügen wir ebenfalls die NumPy-Bibliothek hinzu.\n",
    "Hinzukommt Pythons Objektzähler, der Counter, zum zählen der Wörter.\n",
    "Als letzer Punkt wird, um die (sozialen) Netzwerke als Graph abbilden zu können, die Python-Bibliothek Networkx importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importieren der Reden und anwenden der Lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Reden der Sitzungen vom deutschen Bundestag der 19. Wahlperiode sind in Plenarprotokollen auf der Website des deutschen Bundestages in XML und json Dateien beziehbar. Zum schnelleren Bearbeiten dieser großen Menge an Daten (>24000 Reden), bietet sich für das das jupyter notebook das weit verbreitete und leicht verarbeitbare json-Format an.\n",
    "Die Funktion laden_alleReden soll die vorbereitete json Datei mit dem Inhalt aller Reden laden und gibt eine Kopie der json Datei wieder damit...\n",
    "Die Funktion laden_alleReden öffnet und ließt durch Aufruf des Kürzels fp das angegebene Verzeichnis. Dadurch wird der Inalt des Verzeichnisses geladen, in data gespeichert und eine Kopie für das Notebook erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laden_alleReden():\n",
    "    \n",
    "    with open('../data/speeches_preprocessed.json', 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    \n",
    "    return data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion laden_alleReden wird nun mit alleReden aufgerufen und packt den Return in die Liste??? alleReden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alleReden = laden_alleReden()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reden nach Parteien sortieren und zu Parteitexten zusammenfügen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeder Parteiname wird als ein Key deklariert und erhält ein Array, in dem man die Reden die zu dieser Partei gehören ablegen kann. Mittels der Attribute Redner und parteizugehörigkeit, die die gebündelten Reden in der json-Datei besitzen, werden alle Reden auf die jeweils zugehörige Partei aufgeteilt und dort zu einer Parteiliste ergänzt, bis alle Reden verteilt wurden.\n",
    "Zur erheblichen Reduzierung der Rechenzeiten, wurden auf dem Text zuvor Lemmata angewendet???, um Mehrfachauftreten von Wörtern durch angrenzende Satzzeichen zu verhindern.\n",
    "Protokollseitig wurden die Parteinamen nicht immer identisch notiert, weshalb abweichende Namensvariationen zusätzlich den üblichen Parteibezeichnungen zugewiesen werden müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reden_gefiltert = {'CDU_CSU':[],\n",
    "                   'SPD':[],\n",
    "                   'AfD':[],\n",
    "                   'FDP':[],\n",
    "                   'BÜNDNIS 90_DIE GRÜNEN':[],\n",
    "                   'DIE LINKE':[],\n",
    "                   'fraktionslos':[],\n",
    "                   'Bremen':[]\n",
    "                  }\n",
    "\n",
    "for rede in alleReden:\n",
    "    rede['party']=rede['party'].replace(u'\\xa0', u' ')\n",
    "    rede['party']=rede['party'].replace(u'/', u'_')\n",
    "    if rede['party']=='Bündnis 90_Die Grünen':\n",
    "        rede['party']='BÜNDNIS 90_DIE GRÜNEN'\n",
    "    if rede['party']=='Fraktionslos':\n",
    "        rede['party']='fraktionslos'\n",
    "    \n",
    "    reden_gefiltert[ rede['party'] ].extend(rede['text_lem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## erstellen von unique words (uw) von Einzelparteien und Parteipaaren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dieser Funktion sollen jene Wörter gefunden werden, die ausschließlich von einer Partei stammen und somit ihr Merkmal sind. Diese zuweisbaren Wörter werden im Folgenden unique words genannt. Das Herausfiltern dieser \"charakteristischen\" Wörter geschieht mit Hilfe einer einfachen Mengensubtraktion.\n",
    "Die Funktion verwendet die Parteinamen in Form der erstellten Keys und bildet zwei Wörterbücher in Form von Sets.\n",
    "Das eine enthält Reden der zu untersuchenden Partei(Key) und das andere die wieder gebündelten Reden der restlichen Parteien.\n",
    "Die unique words der zu untersuchenden Partei sind das Resultat der Subtraktion der Wörter aus der Menge der eigenen Parteireden minus der Wörter aus der Menge der Parteireden der restlichen Parteien.\n",
    "Am Ende werden die unique Words in Form von Listen ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unique_words(partylist,word_dict):\n",
    "\n",
    "    andereworte = set()\n",
    "    parteienworte = set()\n",
    "    for key in word_dict.keys():\n",
    "        if key not in partylist:\n",
    "            andereworte = andereworte | set(word_dict[key])\n",
    "        else:\n",
    "            parteienworte = parteienworte | set(word_dict[key])\n",
    "    \n",
    "    \n",
    "\n",
    "    unique_words = parteienworte - andereworte\n",
    "    \n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unique words nach Parteien in Form von Keys sortiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dem Set uw_all werden nun die unique words aller Parteien heruasgefiltert und unter ihren Partei Keys abgelegt, wie eine sortierte Tabelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uw_all = {'SPD': return_unique_words('SPD',reden_gefiltert),\n",
    "'FDP': return_unique_words('FDP',reden_gefiltert),\n",
    "'CDU_CSU': return_unique_words('CDU_CSU',reden_gefiltert),\n",
    "'DIE LINKE': return_unique_words('DIE LINKE',reden_gefiltert),\n",
    "'BÜNDNIS 90_DIE GRÜNEN': return_unique_words('BÜNDNIS 90_DIE GRÜNEN',reden_gefiltert),\n",
    "'AfD': return_unique_words('AfD',reden_gefiltert)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die gesammten unique Words einer Partei lassen sich jetzt mit Angabe des Partei Keys öffnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uw_all['SPD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Liste der unique words einer jeden Partei wird als json Datei im Verzeichnis bereitgestellt.\n",
    "Zuerst wird eine Leere Menge erschaffen, die Listen werden zurückgegeben, mit dem Inhalt, der bei uw_all unter dem Parteinamen als Key gespeichert ist.\n",
    "Mittels der open Funktion wird ermöglicht bei eingabe des Kürzels unter eingetragenem Dateipfad eine json Datei zu schreiben. In dieser json Datei wird dann alles aus der Liste aus dem oberen Teil abgespeichert und im Verzeichnis abgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uw_all_lists = {}\n",
    "for key in uw_all:\n",
    "    uw_all_lists.update({ key : list(uw_all[key]) })\n",
    "\n",
    "with open('../data/unique_words_per_party.json', 'w') as fp:\n",
    "    json.dump(uw_all_lists, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## erstellen von shared unique words (suw) von Parteipaaren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um weitere Charakteristika in der Semantik parteiübergeordneter politischer Blöcke zu untersuchen, werden die gemeinsamen unique words paarweiser Parteien erstellt. Also erst eine Schnittmenge der Wörter die zwei Parteien vereinen und dann mit einem gesammelten Wörterbuch der übrigen Parteien vergleichen. Somit lässt sich eventuell eine sprachliche Nähe un dgemeinsamer häufiger Gebrauch bestimmter Wörter zwischen Parteien ausfindig machen.\n",
    "Die unique words listen werden wie ein Schachbrett in Reihe und Spalte aufgelistet und die Zweierkombinationen ertsellt während die unique words Listen der übrigen 4 Parteien wieder gebündelt subtrahiert werden.\n",
    "Dazu wird in aufsteigender Zählweise mit der Partei in der i. Spalte begonnen und für die erstmal alle Kombinationsmöglichkeiten mit den Parteien in den j Reihen erstellt, mit j einer Spaltenzahl höher beginnend, damit keien Schnittmenge mit sich selbst entsteht, bzw. die Diagonal der Tabelle ausgelassen wird. Da die Parteikeys aus der uw_all Liste keinen Zahlen in Rangfolgen entsprechen, wird mit enumerate nach Anzahl der Keys, einem jeden Partei Key eine Zahl zugewiesen unter der er sich dann in Spalte und Reihe wiederfindet.\n",
    "Der print Befehl gibt zudem die Anzahl der shared unique words einer jeden Partei mit den jeweils anderen Parteien wieder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = (len(uw_all), len(uw_all))\n",
    "suw = [[0 for i in range(cols)] for j in range(rows)]\n",
    "suw_words = [[0 for i in range(cols)] for j in range(rows)]\n",
    "for ix,p1 in enumerate(uw_all.keys()):\n",
    "    for jx,p2 in enumerate(uw_all.keys()):\n",
    "        if jx > ix:\n",
    "            print(p1,p2)\n",
    "            tmp = return_unique_words([p1,p2],reden_gefiltert) - return_unique_words(p1,reden_gefiltert) - return_unique_words(p2,reden_gefiltert)\n",
    "            suw_words[ix][jx] = tmp\n",
    "            suw[ix][jx] = len(tmp)\n",
    "            suw[jx][ix] = suw[ix][jx]\n",
    "\n",
    "print(suw)\n",
    "\n",
    "#0 SPD, 1 FDP, 2 CDU/CSU, 3 Linke, 4 Grüne, 5 AfD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die unique words eines bestimmten Parteienpaares lassen sich jetzt mit Angabe der Parteitabellenzahlen öffnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(uw_all.keys())\n",
    "#print(suw_words[0][1])\n",
    "#0 SPD, 1 FDP, 2 CDU/CSU, 3 Linke, 4 Grüne, 5 AfD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud einer einzelnen Partei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wird aus den Listen zur Veranschaulichung eine Wordcloud erstellt. Bei einer Wordcloud entspricht die Schriftgröße der dargestellten Wörter ihrer prozentualen Häufigkeit des Vorkommens in der Liste.\n",
    "Es wird der Partei-Key der zu visualisierenden Partei verwendet.\n",
    "in der Counter Funktion werden die Wörter der unique word Liste der Partei iterativ durhgegangen und bei jedem geschaut, wie oft es im ungekürzten Parteitext vorkommt, der all ihre Reden enthällt um die Häufigkeit des verwendeten Wortes festzustellen.\n",
    "Die imagemask bzw. Formstruktur der Wordcloud wird durch ein selbstgewähltes Bild (Kreis) festgelegt, wobei ein png Dateiformat notwendig ist.\n",
    "Weiter lassen sich als Grundeinstellung ein paar Parameter festlegen, wie Detailgrad und Farben und die Quelle für die anzuzeigenden Wörter in Form der Counterausgabe.\n",
    "Aus den Grundeinstellungen wird jetzt die Cloud, bzw. in diesem Fall der Kreis erzeugt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#party = 'AfD'\n",
    "#party = 'CDU_CSU'\n",
    "#party = 'SPD'\n",
    "#party = 'FDP'\n",
    "#party = 'BÜNDNIS 90_DIE GRÜNEN'\n",
    "party = 'DIE LINKE'\n",
    "\n",
    "uw_counter = [wort for wort in reden_gefiltert[party] if wort in uw_all[party]]\n",
    "counts = Counter(uw_counter)\n",
    "\n",
    "mask = np.array(Image.open(\"circle.png\"))\n",
    "\n",
    "# parameters:\n",
    "\n",
    "wordcloud = WordCloud(width=3200, height=3200,background_color='white', max_words=500, mask=mask,contour_color='#000000',contour_width=0,colormap='rainbow').generate_from_frequencies(counts)\n",
    "\n",
    "\n",
    "# create image as circle:\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# presentate:\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speichern als Rasterbild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genügt die Darstellung den Ansprüchen, ist sie als \"normales Bild\", als Rasterbild im png Format auf dem Dateipfad mit dem jeweiligen Dateinamen am Ende abspeicherbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store to file\n",
    "wordcloud.to_file(f\"../private/wordclouds/wordcloud_{party}.png\")\n",
    "#plt.savefig(f\"../private/wordclouds/wordcloud_{party}.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speichern als SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Zwecke einer qualitätsverlustfreien Vergrößerung der Wordcloud zum pixelfreien Heranzoomen, oder\n",
    "z.B. Anbringung auf Postern, lässt sich die Darstellung auch als Vektorgrafik in Form einer SVG Datei abspeichern. \n",
    "w+ = Datei lesen und schreiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party='DIE GRÜNEN'\n",
    "wordcloud_svg = wordcloud.to_svg(embed_font=True)\n",
    "f = open(f\"../private/wordclouds/wordcloud_{party}.svg\",\"w+\")\n",
    "f.write(wordcloud_svg)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcounterliste und Cloud für Parteipaar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion erstellt eine Wordcloud aller selbst gewählten Parteipaare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 SPD, 1 FDP, 2 CDU/CSU, 3 Linke, 4 Grüne, 5 AfD\n",
    "\n",
    "uw_counter = [wort for wort in reden_gefiltert['SPD'] + reden_gefiltert['FDP'] if wort in suw_words[0][1]]\n",
    "\n",
    "counts = Counter(uw_counter)\n",
    "\n",
    "mask = np.array(Image.open(\"circle.png\"))\n",
    "\n",
    "# parameters:\n",
    "\n",
    "wordcloud = WordCloud(background_color='white', max_words=500, mask=mask,contour_color='#000000',contour_width=3,colormap='rainbow').generate_from_frequencies(counts)\n",
    "#wordcloud = WordCloud(background_color='white', max_words=100, mask=mask).generate_from_frequencies(counts)\n",
    "\n",
    "\n",
    "# create image as circle\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# presentate:\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speichern als SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion zum Abspeichern der Wordcloud der Parteipaare als Vektorgrafik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_svg = wordcloud.to_svg(embed_font=True)\n",
    "f = open(\"../private/wordclouds/wordcloud.svg\",\"w+\")\n",
    "f.write(wordcloud_svg )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als besondere Darstellungsmethode von (sozilen) Netzwerken als Graph, kann ein Netzwerk ausgegeben werden, das sich mit der Software Gephi visualisieren lässt. Dafür wird im jupyter notebook ein Knoten- und Kantennetzwerk aus den unique words Listen erzeugt. Als erstes werden die Knoten erstellt und mit den leeren Attributen Identifikationsnummer, wird erzeugt durch enumerate-Schleife, dazu Parteinamen und der Größe der parteiangehörigen unique words Liste ausgestattet.\n",
    "Append fügt der Liste \"nodes\" jeden Knoten als Item hinzu. Knoten werden befüllt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for px,partei in enumerate(uw_all.keys()):\n",
    "    node = {\n",
    "        'id' : px,\n",
    "        'name' : partei,\n",
    "        'nuw' : len(uw_all[partei])\n",
    "    }\n",
    "    nodes.append(node)\n",
    "    \n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Graph wird erstellt, der ist nicht gerichtet, wird word_graph deklariert, bekommt die vorher erstellten Knoten als nodes und noch zu berechnende Links als Kanten des Graphen.\n",
    "Für die Links bzw. KAnten, werden alle Knoten miteinander verglichen. Es gibt über jede Kante 2 inzidente Knoten, einer als Quelle einer als Ziel, dazu werden Gewichte berechnet, auf BAsis der geteilten unique words (suw).\n",
    "Je mehr geteilte Wörter es gibt, desto stärker ist das Gewicht zwischen den Parteien.\n",
    "Jede berehnete KAnte wird dem Graphen hinzugefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph = {\n",
    "    'directed': False,\n",
    "    'graph': 'word_graph',\n",
    "    'links': [],\n",
    "    'nodes': nodes\n",
    "}    \n",
    "\n",
    "for ix,nodeI in enumerate(graph['nodes']):\n",
    "    for jx,nodeJ in enumerate(graph['nodes']):\n",
    "        if jx > ix:\n",
    "            #print(p1,p2)\n",
    "            #tmp = return_unique_words([p1,p2],reden_gefiltert) - return_unique_words(p1,reden_gefiltert) - return_unique_words(p2,reden_gefiltert)\n",
    "            #suw_words[ix][jx] = tmp\n",
    "            weight = suw[ix][jx]\n",
    "            #suw[jx][ix] = suw[ix][jx]\n",
    "            \n",
    "            source = nodeI['id']\n",
    "            target = nodeJ['id']\n",
    "            link_dict = {\n",
    "                    'source':source,\n",
    "                    'target':target,\n",
    "                    'weight':weight       \n",
    "                }\n",
    "            graph['links'].append(link_dict)\n",
    "    \n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeder Knoten wird an Gephi übergeben, bis alle Knoten fertig sind. Dann folgen die Kanten.\n",
    "Eine XML wird im selben Ordner wie das Notebook abgespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphforgephi = nx.Graph()\n",
    "for node in graph['nodes']:\n",
    "    graphforgephi.add_node(node['id'],name = node['name'],nuw = node['nuw']);\n",
    "    #if(node['name'] == 'Angela Merkel'):\n",
    "    #    print(node['tops'])\n",
    "\n",
    "print('nodes done')\n",
    "for link in (graph['links']):   \n",
    "    graphforgephi.add_edge(link['source'],link['target'],weight = link['weight'])\n",
    "print('links done')   \n",
    "nx.write_gexf(graphforgephi, \"graphforgephi.gexf\")\n",
    "print('save done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
