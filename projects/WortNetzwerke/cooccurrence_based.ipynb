{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for sentences that contain certain words\n",
    "\n",
    "## Import and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import collections\n",
    "\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('de') #load spacy model\n",
    "\n",
    "# for WordClouds \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/speeches_20.jsonl\",'r',encoding = \"utf8\") as fp:\n",
    "    data = list(fp)\n",
    "speeches = []\n",
    "for line in data:\n",
    "    speeches.append(json.loads(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5318"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speeches)\n",
    "#speeches[2776]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_for(what, search_terms, speeches):\n",
    "    filtered_speeches = []\n",
    "    if what == 'text':\n",
    "        search_terms_low = []\n",
    "        for st in search_terms:\n",
    "            search_terms_low.append(st.lower())\n",
    "        for speech in speeches:\n",
    "            match = [st in speech[what].lower() for st in search_terms_low]\n",
    "            if all(st == True for st in match):\n",
    "            #if any(st in speech[what] for st in search_terms):\n",
    "                #print(match)\n",
    "            #if ( search_terms in speech[what] ):\n",
    "                filtered_speeches.append(speech)\n",
    "    else:\n",
    "        for speech in speeches:\n",
    "            if ( speech[what].lower() in set(search_terms_low) ):\n",
    "                filtered_speeches.append(speech)\n",
    "        \n",
    "    filtered_speeches.sort(key = lambda x:x['date'])   \n",
    "    return filtered_speeches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find speeches with focal term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#focal_terms = ['Digitalisierung','Zusammenhalt','Demokratie']\n",
    "#focal_terms = ['Digitalisierung']\n",
    "#focal_terms = ['Plattform','Demokratie']\n",
    "#focal_terms = ['extrem','Plattform']\n",
    "#focal_terms = ['plattform','demokratie']\n",
    "#focal_terms = ['Plattform','Meinung']\n",
    "#focal_terms = ['sozial','Netzwerk','Meinung']\n",
    "#focal_terms = ['social']\n",
    "focal_terms = ['Grundeinkommen']\n",
    "#subset = filter_for('party', ['CDU/CSU'], speeches)\n",
    "subset = filter_for('text', focal_terms, speeches)\n",
    "len(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Find sentences with focal term\n",
    "\n",
    "Note: this requires spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ralph Brinkhaus CDU/CSU\n",
      "Das ist die Vorstufe zum bedingungslosen Grundeinkommen.\n",
      "Hermann Gröhe CDU/CSU\n",
      "einen schrittweisen Weg in ein bedingungsloses Grundeinkommen lehnen wir entschieden ab.\n",
      "Johannes Huber Fraktionslos\n",
      "In Wahrheit versucht die Ampel, hier den ersten Schritt zum bedingungslosen Grundeinkommen zu unternehmen.\n",
      "Hubertus Heil SPD\n",
      "Frau Klose, ich danke Ihnen für die Frage, weil mir das die Gelegenheit gibt, dem Kollegen vielleicht noch einmal zu sagen: Wir führen kein bedingungsloses Grundeinkommen ein, sondern wir führen ein soziales Bürgergeld ein.\n",
      "Jens Teutrine FDP\n",
      "„Warum es kein bedingungsloses Grundeinkommen light gibt“.\n",
      "Markus Reichel CDU/CSU\n",
      "Wir als Union stehen zu genau dieser Leistungsgerechtigkeit und lehnen deswegen auch ein Sanktionsmoratorium und damit quasi den Einstieg in ein bedingungsloses Grundeinkommen ab.\n",
      "Jens Teutrine FDP\n",
      "Sie sagen, wir würden ein bedingungsloses Grundeinkommen einführen, ohne selbst einen Vorschlag zu machen.\n",
      "Sie behaupten immer wieder, das Bürgergeld sei ein bedingungsloses Grundeinkommen.\n",
      "Alice Weidel AfD\n",
      "Sie funktionieren die Hartz-IV-Leistungen durch Streichung der Sanktionsmöglichkeiten zum De-facto-Grundeinkommen um, und Sie wollen den Zugang zu diesen Leistungen im nächsten Schritt nicht nur für ukrainische Kriegsflüchtlinge, sondern auch für alle Asyleinwanderer öffnen.\n",
      "Silke Launert CDU/CSU\n",
      "„Leistung muss sich lohnen, Mitwirkung muss sich lohnen“, hin zum bedingungslosen Grundeinkommen.\n",
      "Keine Sanktionen, am besten bedingungsloses Grundeinkommen.\n",
      "René Springer AfD\n",
      "Anstatt nun die zu entlasten, die es verdient haben, bauen Sie gerade unseren Sozialstaat radikal um und führen ein Grundeinkommen für Asylbewerber ein.\n",
      "Marc Biadacz CDU/CSU\n",
      "Auf alle Fälle hat sich der Minister bemüht, die Hartz‑IV-Sanktionen schnell auszusetzen, um den Weg für ein bedingungsloses Grundeinkommen frei zu machen.\n",
      "Hermann Gröhe CDU/CSU\n",
      "Der Landkreistag warnt zu Recht\n",
      "                    eindringlich vor Schritten hin zu einem bedingungslosen Grundeinkommen.\n",
      "Markus Kurth BÜNDNIS 90/DIE GRÜNEN\n",
      "Das andere ist: Wir werden hier kein bedingungsloses Grundeinkommen einführen und die Sanktionen abschaffen;\n",
      "Jürgen Pohl AfD\n",
      "Gemeint ist ein Grundeinkommen ohne Sanktionsmöglichkeit der Arbeitsverwaltung.\n",
      "Ulrike Schielke-Ziesing AfD\n",
      "Die einzige Antwort ist,\n",
      "                    dass hier sämtliche Steuerungssysteme versagen und Fehlanreize wirken, die nun auch noch zementiert werden sollen, indem sich das geplante Bürgergeld zunehmend\n",
      "                    als Einstieg in das bedingungslose Grundeinkommen entpuppt.\n",
      "Martin Rosemann SPD\n",
      "Was die Union wieder behauptet hat, nämlich dass wir damit ein bedingungsloses Grundeinkommen schaffen würden und dass wir Sanktionen und Mitwirkungspflichten abschaffen würden, wird durch Wiederholung nicht wahrer.\n",
      "Jana Schimke CDU/CSU\n",
      "Sie führen ein bedingungsloses Grundeinkommen ein.\n",
      "Stephanie Aeffner BÜNDNIS 90/DIE GRÜNEN\n",
      "Und dann bringen Sie faktenfreie Behauptungen in die Debatte ein wie die, wir führten ein bedingungsloses Grundeinkommen ein.\n",
      "Jörn König AfD\n",
      "Denn uferloses Gelddrucken, bedingungsloses Grundeinkommen und sanktionsloses Hartz\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "#focus = 'social'\n",
    "focus = 'Grundeinkommen'\n",
    "\n",
    "for rede in subset:\n",
    "    doc = nlp(rede[\"text\"])\n",
    "    print(rede['name'],rede['party'])\n",
    "    for sent in doc.sents:\n",
    "        #print(sent.text)\n",
    "        #if all(ft.lower() in sent.text.lower() for ft in focal_terms):\n",
    "        #if focal_terms[0].lower() in sent.text.lower() and focal_terms[1].lower() in sent.text.lower():\n",
    "        #for focus in focal_terms:    \n",
    "        if focus.lower() in sent.text.lower():        \n",
    "            sentences.append(sent)\n",
    "            print(sent)\n",
    "        \n",
    "#len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Section\n",
    "\n",
    "## Construct a network of word-cooccurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantPOS = ['NOUN','ADJ','PROPN']\n",
    "sentencesNN = []\n",
    "words = []\n",
    "for sen in sentences:\n",
    "    lem = []\n",
    "    for token in sen:\n",
    "        if token.pos_ in relevantPOS:\n",
    "            lem.append(token.lemma_)\n",
    "    sentencesNN.append(lem)\n",
    "    words.extend(lem)\n",
    "#sentencesNN\n",
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306 306\n"
     ]
    }
   ],
   "source": [
    "nodes=[]\n",
    "curid=1\n",
    "for word in set(words):\n",
    "    node = {\n",
    "    'id' :  curid,\n",
    "    'name' : word\n",
    "    }\n",
    "    nodes.append(node)\n",
    "    curid=curid+1\n",
    "    \n",
    "graph = {\n",
    "    'directed': False,\n",
    "    'graph': 'word_graph',\n",
    "    'links': [],\n",
    "    'nodes': nodes\n",
    "}\n",
    "\n",
    "links = []\n",
    "linkedwords =[]\n",
    "linkedids =[]\n",
    "lx = 0;\n",
    "for wx1,w1 in enumerate(nodes):\n",
    "    #print(wx1)\n",
    "    for wx2,w2 in enumerate(nodes):\n",
    "        if(w2['id'] > w1['id']):\n",
    "            for sen in sentencesNN:\n",
    "                if w1['name'] in sen and w2['name'] in sen:\n",
    "                    weight = len([ele for ele in linkedwords if ele == (' '.join([w1['name'],w2['name']]))])\n",
    "                    #    links[]\n",
    "                    #else:\n",
    "                    #print(weight)\n",
    "                    linkedwords.append(' '.join([w1['name'],w2['name']]))\n",
    "                    #linkedids[ w1['id'] ] , w2['id'] )\n",
    "                    link_dict = {\n",
    "                    'source':w1['id'],\n",
    "                    'target':w2['id'],\n",
    "                    'sourceWD':w1['name'],\n",
    "                    'targetWD':w2['name'],\n",
    "                    'weight': weight+1      \n",
    "                    }\n",
    "                    #print(link_dict)\n",
    "                    links.append(link_dict)\n",
    "                    graph['links'].append(link_dict)\n",
    "               \n",
    "                    #for link in links:\n",
    "\n",
    "#linkedwords\n",
    "zähler = collections.Counter(linkedwords).most_common()\n",
    "print(len(linkedwords),len(links))\n",
    "#graph['links']=links\n",
    "#print(zähler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    if link['weight']>0:\n",
    "        graph['links'].append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612 66\n"
     ]
    }
   ],
   "source": [
    "print(len(graph['links']),\n",
    "len(graph['nodes']))\n",
    "#for link in graph['links']:\n",
    "#    if link['weight']>1:\n",
    "        #print(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store as HTML graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {'data':graph, \n",
    "        'nodecoloring':'party', \n",
    "        'nodelabel': 'name', \n",
    "        #'nodelabel': 'mfic',\n",
    "        \"darkmode\": False,\n",
    "        \"edgevisibility\": True,\n",
    "        \"particles\": False\n",
    "       }\n",
    "result = requests.post('https://penelope.vub.be/network-components/visualiser', json=json)\n",
    "\n",
    "with open (f\"./wordnet.html\", \"w\") as f:\n",
    "    f.write(result.json()['graph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store as Gephi graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:00<00:00, 320611.72it/s]\n",
      "100%|██████████| 3550/3550 [00:00<00:00, 509993.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes done\n",
      "links done\n",
      "save done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import tqdm\n",
    "\n",
    "graphforgephi = nx.Graph()\n",
    "for node in tqdm.tqdm(graph['nodes']):\n",
    "    graphforgephi.add_node(node['id'],name = node['name']);\n",
    "print('nodes done')\n",
    "for link in tqdm.tqdm(graph['links']):\n",
    "    #weight = all((' '.join([w1['name'],w2['name']]) in linkedwords)\n",
    "    #print(weight)         \n",
    "    graphforgephi.add_edge(link['source'],link['target'],weight=link['weight'])\n",
    "print('links done')   \n",
    "nx.write_gexf(graphforgephi, \"graphforgephi.gexf\")\n",
    "print('save done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
