{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funktionen zum Durchsuchen und Filtern der Reden\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Liste enthält 10791 Reden\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "\n",
    "# Das (natural language toolkit) müsst ihr bestimmt installieren.\n",
    "# Weiß noch jemand, wie das geht?\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('de') #load spacy model\n",
    "\n",
    "# Die collections.Counter package müsst ihr bestimmt auch installieren.\n",
    "# Die brauchen wir später, um Worte zu zählen.\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Hier legen wir fest, welche Daten (Wahlperiode 19 oder 20) wir laden:\n",
    "legislatur = 20\n",
    "\n",
    "# Wir generieren eine leere Liste:\n",
    "alleReden = []\n",
    "\n",
    "# Wir öffnen den entsprechende File (Dateipfad anpassen!):\n",
    "with jsonlines.open(f'../../data/speeches_{legislatur}.jsonl') as f:\n",
    "    for line in f.iter():\n",
    "    # Wir packen alles Zeile für Zeile zu unserer Liste:\n",
    "        alleReden.append(line)\n",
    "        \n",
    "# Wir sortieren nach Datum:\n",
    "alleReden.sort(key = lambda x:x['date'])\n",
    "\n",
    "# Wir lassen uns zeigen, wie viele Reden enthalten sind.\n",
    "print(f'Die Liste enthält {len(alleReden)} Reden')\n",
    "# Neue Zeile auf der Konsole:\n",
    "print(f'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suchfunktionen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diese Liste (Suche nach letzte Generation) enthält 2 Reden.\n",
      "\n",
      "\n",
      "Diese Liste (Suche nach letzte Generation) enthält 2 Sätze\n",
      "\n",
      "\n",
      "Diese Liste (Suche nach Olaf Scholz) enthält 171 Reden\n",
      "\n",
      "\n",
      "In 4 Reden gibt es 6 Sätze von Olaf Scholz, die Klimakrise enthalten:\n",
      "\n",
      "\n",
      " Satz 1: Die Klimakrise erfordert entschlossenes, systematisches und international abgestimmtes Vorgehen.\n",
      "\n",
      "\n",
      " Satz 2: Deshalb bieten wir China Zusammenarbeit an bei Menschheitsherausforderungen wie der Klimakrise, der Pandemie oder der Rüstungskontrolle.\n",
      "\n",
      "\n",
      " Satz 3: Vor uns liegen enorme Aufgaben: die wirtschaftliche Transformation voranbringen, die Klimakrise in den Griff bekommen, demografischen Wandel gestalten, Frieden in Europa sichern.\n",
      "\n",
      "\n",
      " Satz 4: Aber jetzt ist klar: Die Erneuerbaren brauchen wir nicht nur wegen der Klimakrise.\n",
      "\n",
      "\n",
      " Satz 5: Angesichts von Krieg und Klimakrise geht es heute auch um Europa selbst.\n",
      "\n",
      "\n",
      " Satz 6: Meine Damen und Herren, wir leben in einer besonders herausfordernden Zeit: Die Klimakrise spitzt sich zu; der Krieg ist zurück in Europa; die globalen Gewichte verschieben sich.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Zunächst brauchen wir eine Funktion, die uns die Reden gibt, die ein bestimmtes Wort enthalten. \n",
    "#  Funktion für Textsuche: \n",
    "#  Gibt eine Untermenge an Reden zurück, die einen bestimmten String (Wort) enthalten.\n",
    "\n",
    "def find_speeches_with_word(search_term, speeches):\n",
    "    filtered_speeches = []\n",
    "    for speech in speeches:\n",
    "        if ( search_term in speech['text'] ):\n",
    "            filtered_speeches.append(speech)\n",
    "    return filtered_speeches\n",
    "\n",
    "## Probieren wir das mal aus.\n",
    "#  Das ist unser Suchwort (oder String):\n",
    "wort = 'letzte Generation'\n",
    "#wort = 'Kapital'\n",
    "\n",
    "#  Hier rufen wir die Suchfunktion auf und speichern die Untermenge der Reden.\n",
    "untermenge = find_speeches_with_word(wort, alleReden)\n",
    "\n",
    "# Wir lassen uns zeigen, wie viele Reden enthalten sind.\n",
    "print(f'Diese Liste (Suche nach {wort}) enthält {len(untermenge)} Reden.')\n",
    "# Neue Zeile auf der Konsole:\n",
    "print(f'\\n')\n",
    "\n",
    "## Reden sind lang und die Worte tauchen in verschiedenen Kontexten auf.\n",
    "#  Wir würden gerne alle Sätze sehen, in denen der Suchbegriff vorkommt.\n",
    "#  Aber natürlich kommt unser Suchstring nur in Sätzen vor, die in de Untermenge an Reden sind. \n",
    "\n",
    "def find_sentences_with_word(search_term, speeches):\n",
    "    sents_with_words = []\n",
    "    for speech in speeches:\n",
    "        sent_list = nltk.sent_tokenize(speech['text'])\n",
    "        for sent in sent_list:\n",
    "            if search_term in sent:\n",
    "                sents_with_words.append(sent)\n",
    "    return sents_with_words\n",
    "                \n",
    "# Probieren wir diese Funktion einmal aus:\n",
    "satz_liste = find_sentences_with_word(wort,untermenge)\n",
    "\n",
    "# Wir lassen uns zeigen, wie viele Sätze in der Liste enthalten sind.\n",
    "print(f'Diese Liste (Suche nach {wort}) enthält {len(satz_liste)} Sätze')\n",
    "# Neue Zeile auf der Konsole:\n",
    "print(f'\\n')\n",
    "\n",
    "# Wollen wir uns die alle anzeigen lassen? Ja oder Nein?\n",
    "wir_wollen = False\n",
    "if wir_wollen:\n",
    "    for satz in satz_liste:\n",
    "        print(satz)\n",
    "\n",
    "        \n",
    "## Nun wäre es doch spannend, die Reden einer Partei oder eines Politikers zu sehen.\n",
    "#  Dazu entwickeln wir eine Funktion, die es erlaubt, in den anderen Felder (keys) zu suchen.\n",
    "#  Funktion, mit der man eine Menge an Reden nach verschiedenen Kriterien filtern kann.\n",
    "#  Es wird die entsprechende Untermenge zurückgegeben.\n",
    "#  'what' enthält den Key, wo gesucht werden soll. Interessant vor allem: 'name' und 'party'\n",
    "\n",
    "def filter_speeches_for(what, search_term, speeches):\n",
    "    filtered_speeches = []\n",
    "    for speech in speeches:\n",
    "        if search_term in speech[what]:\n",
    "            filtered_speeches.append(speech)\n",
    "        \n",
    "    filtered_speeches.sort(key = lambda x:x['date'])   \n",
    "    return filtered_speeches\n",
    "\n",
    "# Beispiel: Für alle Reden von Olaf Scholz:\n",
    "suche_nach = 'Olaf Scholz'\n",
    "untermenge = filter_speeches_for('name', suche_nach, alleReden)\n",
    "# Wir lassen uns zeigen, wie viele Reden enthalten sind.\n",
    "print(f'Diese Liste (Suche nach {suche_nach}) enthält {len(untermenge)} Reden')\n",
    "print(f'\\n')\n",
    "\n",
    "####\n",
    "## Jetzt könnten wir die Sätze von Olaf Scholz mit einem bestimmten Wort anschauen.\n",
    "#  Das ist unser Suchwort (oder String):\n",
    "#wort = 'Klimawandel'\n",
    "wort = 'Klimakrise'\n",
    "\n",
    "#  Hier rufen wir die Suchfunktion auf und speichern die Untermenge der Reden.\n",
    "untermenge = find_speeches_with_word(wort, alleReden)\n",
    "\n",
    "# Wir suchen nach:\n",
    "suche_nach = 'Olaf Scholz'\n",
    "untermenge = filter_speeches_for('name', suche_nach, untermenge)\n",
    "satz_liste = find_sentences_with_word(wort,untermenge)\n",
    "\n",
    "print(f'In {len(untermenge)} Reden gibt es {len(satz_liste)} Sätze von {suche_nach}, die {wort} enthalten:')\n",
    "print(f'\\n')\n",
    "\n",
    "# Die schauen wir uns an.\n",
    "for sx,satz in enumerate(satz_liste):\n",
    "    print(f' Satz {sx+1}: {satz}')\n",
    "    print(f'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2072 Reden gibt es 4906 Sätze, die Klima enthalten:\n",
      "\n",
      "\n",
      "In der Liste befinden sich 54076 Worte: \n",
      "\n",
      "[('Klimaschutz', 1245), ('Klimakrise', 509), ('Klimawandel', 388), ('Klima', 382), ('müssen', 369), ('mehr', 340), ('Klima-', 262), ('Euro', 228), ('Deutschland', 220), ('Menschen', 210), ('Klimaziele', 209), ('beim', 198), ('geht', 193), ('erreichen', 181), ('Milliarden', 171), ('Klimawandels', 169), ('ganz', 168), ('brauchen', 163), ('schon', 162), ('unserer', 160)]\n"
     ]
    }
   ],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "\n",
    "german_stop_words = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "####\n",
    "## Lass mal was versuchen:\n",
    "#wort = 'Kapitalismus'\n",
    "#wort ='Nachhaltigkeit'\n",
    "\n",
    "wort ='Klima'\n",
    "\n",
    "#  Hier rufen wir die Suchfunktion auf und speichern die Untermenge der Reden.\n",
    "untermenge = find_speeches_with_word(wort, alleReden)\n",
    "satz_liste = find_sentences_with_word(wort,untermenge)\n",
    "\n",
    "#print(f'In {len(untermenge)} Reden gibt es {len(satz_liste)} Sätze von {suche_nach}, die {wort} enthalten:')\n",
    "print(f'In {len(untermenge)} Reden gibt es {len(satz_liste)} Sätze, die {wort} enthalten:')\n",
    "print(f'\\n')\n",
    "\n",
    "# Die schauen wir uns an und generieren eine Wortliste\n",
    "wort_liste = []\n",
    "for sx,satz in enumerate(satz_liste):\n",
    "    #print(f' Satz {sx+1}: {satz}')\n",
    "    worte = nltk.word_tokenize(satz)\n",
    "    #print(f' Satz {sx+1}: {worte}')\n",
    "    wort_liste.extend(worte)\n",
    "\n",
    "wort_liste_clean = []\n",
    "for wort in wort_liste:\n",
    "    if wort not in german_stop_words:\n",
    "        if len(wort)>3:\n",
    "            wort_liste_clean.append(wort)\n",
    "    \n",
    "print(f'In der Liste befinden sich {len(wort_liste_clean)} Worte: \\n')\n",
    "#print(wort_liste_clean)\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter(wort_liste_clean)\n",
    "print(counts.most_common(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "####\n",
    "## Lass mal was versuchen:\n",
    "wort = 'Kapitalismus'\n",
    "#wort ='Nachhaltigkeit'\n",
    "wort='Krise'\n",
    "\n",
    "#  Hier rufen wir die Suchfunktion auf und speichern die Untermenge der Reden.\n",
    "untermenge = find_speeches_with_word(wort, alleReden)\n",
    "\n",
    "suche_nach = 'Olaf Scholz'\n",
    "untermenge = filter_speeches_for('name', suche_nach, alleReden)\n",
    "\n",
    "satz_liste = find_sentences_with_word(wort,untermenge)\n",
    "\n",
    "print(f'In {len(untermenge)} Reden gibt es {len(satz_liste)} Sätze von {suche_nach}, die {wort} enthalten:')\n",
    "#print(f'In {len(untermenge)} Reden gibt es {len(satz_liste)} Sätze, die {wort} enthalten:')\n",
    "print(f'\\n')\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Die schauen wir uns an und generieren eine Wortliste\n",
    "def get_list_of_lemmata(satz_liste):\n",
    "    wort_liste = []\n",
    "    for sx,satz in enumerate(satz_liste):\n",
    "        #print(f' Satz {sx+1}: {satz}')\n",
    "        worte = nltk.word_tokenize(satz)\n",
    "        #print(f' Satz {sx+1}: {worte}')\n",
    "        wort_liste.extend(worte)\n",
    "\n",
    "    wort_liste_clean = []\n",
    "    for wort in wort_liste:\n",
    "        if wort not in german_stop_words:\n",
    "            if len(wort)>3:\n",
    "                wort_liste_clean.append(wort)\n",
    "    return wort_liste_clean\n",
    "        \n",
    "\n",
    "# Daselbe mit Spacy\n",
    "def get_list_of_lemmata_spacy(satz_liste):\n",
    "    wort_liste = []\n",
    "    for sx,satz in enumerate(satz_liste):\n",
    "        #print(f' Satz {sx+1}: {satz}')\n",
    "        doc = nlp(satz)\n",
    "        for token in doc:\n",
    "            if token.pos_ in ['NOUN']:\n",
    "                #print(token.lemma_)\n",
    "                wort_liste.append(token.lemma_)    \n",
    "    return wort_liste\n",
    "    \n",
    "#print(wort_liste)    \n",
    "\n",
    "counts = Counter(wort_liste).most_common(20)\n",
    "print(counts[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Search step 1: Klima \n",
      "\n",
      "--> Die Suche nach Klima ergab 230 Reden\n",
      "Add Klimaschutz\n",
      "Add Klimawandel\n",
      "Add Euro\n",
      "Add Herr\n",
      "\n",
      "\n",
      "Search step 2: Klimaschutz \n",
      "\n",
      "--> Die Suche nach Klimaschutz ergab 50 Reden\n",
      "Add Bundesregierung\n",
      "Add Klimaschutzes\n",
      "Add Artenschutz\n",
      "Add Energie\n",
      "\n",
      "\n",
      "Search step 3: Klimawandel \n",
      "\n",
      "--> Die Suche nach Klimawandel ergab 30 Reden\n",
      "Add Klimawandels\n",
      "Add Million\n",
      "Add Anpassung\n",
      "\n",
      "\n",
      "Search step 4: Euro \n",
      "\n",
      "--> Die Suche nach Euro ergab 631 Reden\n",
      "Add Milliarde\n",
      "Add Jahr\n",
      "\n",
      "\n",
      "Search step 5: Herr \n",
      "\n",
      "\n",
      "\n",
      "Search step 6: Bundesregierung \n",
      "\n",
      "--> Die Suche nach Bundesregierung ergab 455 Reden\n",
      "Add Land\n",
      "\n",
      "\n",
      "Search step 7: Klimaschutzes \n",
      "\n",
      "--> Die Suche nach Klimaschutzes ergab 5 Reden\n",
      "\n",
      "\n",
      "Search step 8: Artenschutz \n",
      "\n",
      "--> Die Suche nach Artenschutz ergab 13 Reden\n",
      "Add Artenschutzes\n",
      "Add Energiewende\n",
      "\n",
      "\n",
      "Search step 9: Energie \n",
      "\n",
      "--> Die Suche nach Energie ergab 434 Reden\n",
      "Add Energiepolitik\n",
      "Add Energieversorgung\n",
      "Add Energiepreise\n",
      "\n",
      "\n",
      "Search step 10: Klimawandels \n",
      "\n",
      "--> Die Suche nach Klimawandels ergab 6 Reden\n",
      "Add Bekämpfung\n",
      "\n",
      "\n",
      "Search step 11: Million \n",
      "\n",
      "--> Die Suche nach Million ergab 413 Reden\n",
      "Add Mensch\n",
      "\n",
      "\n",
      "Search step 12: Anpassung \n",
      "\n",
      "--> Die Suche nach Anpassung ergab 20 Reden\n",
      "Add Dame\n",
      "Add Prozent\n",
      "\n",
      "\n",
      "Search step 13: Milliarde \n",
      "\n",
      "--> Die Suche nach Milliarde ergab 316 Reden\n",
      "\n",
      "\n",
      "Search step 14: Jahr \n",
      "\n",
      "\n",
      "\n",
      "Search step 15: Land \n",
      "\n",
      "--> Die Suche nach Land ergab 672 Reden\n",
      "Add Landwirt\n",
      "Add Bürger\n",
      "\n",
      "\n",
      "Search step 16: Artenschutzes \n",
      "\n",
      "--> Die Suche nach Artenschutzes ergab 3 Reden\n",
      "\n",
      "\n",
      "Search step 17: Energiewende \n",
      "\n",
      "--> Die Suche nach Energiewende ergab 105 Reden\n",
      "\n",
      "\n",
      "Search step 18: Energiepolitik \n",
      "\n",
      "--> Die Suche nach Energiepolitik ergab 110 Reden\n",
      "Add Welt\n",
      "\n",
      "\n",
      "Search step 19: Energieversorgung \n",
      "\n",
      "--> Die Suche nach Energieversorgung ergab 78 Reden\n",
      "Add Regierung\n",
      "Add Winter\n",
      "Add Kernkraft\n",
      "\n",
      "\n",
      "Search step 20: Energiepreise \n",
      "\n",
      "--> Die Suche nach Energiepreise ergab 90 Reden\n",
      "Add Energiepreisen\n",
      "Add Inflation\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Let's try an iterative process:\n",
    "\n",
    "import random\n",
    "\n",
    "search_depth = 20\n",
    "sentence_limit = 20400\n",
    "scope = 5\n",
    "\n",
    "#suche_nach = 'SPD'\n",
    "#reden = filter_speeches_for('party', suche_nach, alleReden)\n",
    "reden = alleReden\n",
    "\n",
    "nodes = []\n",
    "neighborhoods = {}\n",
    "\n",
    "start_wort = 'Kapitalismus'\n",
    "start_wort ='Nachhaltigkeit'\n",
    "start_wort ='Krise'\n",
    "start_wort ='Demokratie'\n",
    "start_wort ='Plattform'\n",
    "start_wort ='Medien'\n",
    "start_wort ='Klima'\n",
    "#start_wort ='müssen'\n",
    "\n",
    "partei = 'AfD'\n",
    "\n",
    "nodes.append(start_wort)\n",
    "for step in range(0,search_depth):\n",
    "    print('\\n')\n",
    "    print(f'Search step {step+1}: {nodes[step]} \\n')\n",
    "    wort = nodes[step]\n",
    "    untermengeP = filter_speeches_for('party', partei, reden)\n",
    "    untermengeW = find_speeches_with_word(wort, untermengeP)\n",
    "    if len(untermengeW)/len(untermengeP) < 0.6:\n",
    "        satz_liste = find_sentences_with_word(wort,untermengeW)\n",
    "        print(f'--> Die Suche nach {wort} ergab {len(untermengeW)} Reden')\n",
    "        if(len(satz_liste) > sentence_limit):\n",
    "            print(f'--> Die Suche nach {wort} ergab {len(satz_liste)} Sätze')\n",
    "            neighborhoods.update({wort : []})\n",
    "            #satz_liste = random.sample(satz_liste,sentence_limit)\n",
    "        else:\n",
    "            wort_liste = get_list_of_lemmata_spacy(satz_liste)\n",
    "            \n",
    "            counts = Counter(wort_liste).most_common(scope)\n",
    "            neighbors = []\n",
    "            for ele in counts:\n",
    "                new_word = ele[0]\n",
    "                if new_word != wort and ele[1]>1:\n",
    "                    neighbors.append(new_word)\n",
    "                    if new_word not in nodes:\n",
    "                        nodes.append(new_word)\n",
    "                        print(f'Add {new_word}')\n",
    "                    neighborhoods.update({wort : neighbors})\n",
    "                else:\n",
    "                    neighborhoods.update({wort : []})\n",
    "    else:\n",
    "        neighborhoods.update({wort : []})\n",
    "\n",
    "#print(nodes)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Klima', 'Klimaschutz', 'Klimawandel', 'Euro', 'Herr', 'Bundesregierung', 'Klimaschutzes', 'Artenschutz', 'Energie', 'Klimawandels', 'Million', 'Anpassung', 'Milliarde', 'Jahr', 'Land', 'Energiewende', 'Energiepolitik', 'Energieversorgung', 'Energiepreise'])\n",
      "['Klima', 'Klimaschutz', 'Klimawandel', 'Euro', 'Herr', 'Bundesregierung', 'Klimaschutzes', 'Artenschutz', 'Energie', 'Klimawandels', 'Million', 'Anpassung', 'Milliarde', 'Jahr', 'Land', 'Artenschutzes', 'Energiewende', 'Energiepolitik', 'Energieversorgung', 'Energiepreise', 'Bekämpfung', 'Mensch', 'Dame', 'Prozent', 'Landwirt', 'Bürger', 'Welt', 'Regierung', 'Winter', 'Kernkraft', 'Energiepreisen', 'Inflation']\n"
     ]
    }
   ],
   "source": [
    "print(neighborhoods.keys())\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:00, 112598.77it/s]\n",
      "20it [00:00, 41568.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes done\n",
      "links done\n",
      "save done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "\n",
    "graphforgephi = nx.Graph()\n",
    "\n",
    "for ix,node in tqdm.tqdm(enumerate(nodes)):\n",
    "    #graphforgephi.add_node(ix+1,name = node);\n",
    "    graphforgephi.add_node(node,name = node);\n",
    "    \n",
    "print('nodes done')\n",
    "\n",
    "for ix,node in tqdm.tqdm(enumerate(nodes[0:search_depth])):\n",
    "    for neighbor in neighborhoods[node]:\n",
    "        graphforgephi.add_edge(node,neighbor,weight=1)\n",
    "print('links done')   \n",
    "nx.write_gexf(graphforgephi, \"graphforgephi.gexf\")\n",
    "print('save done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
