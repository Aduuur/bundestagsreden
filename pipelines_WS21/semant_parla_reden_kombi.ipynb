{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "#import collections\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/speeches_preprocessed.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alleReden = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reden_red = []\n",
    "for rede in alleReden:\n",
    "    if len(rede['text_lem']) > 300:\n",
    "        reden_red.append(rede)\n",
    "alleReden = reden_red\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 19657/19657 [00:03<00:00, 4934.01it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "consider = ['NOUN']\n",
    "for rede in tqdm(alleReden):\n",
    "    rel_lemmata = [ ele for ex,ele in enumerate(rede['text_lem']) if rede['text_pos'][ex] in consider ]\n",
    "    rede['text_lem'] = rel_lemmata\n",
    "    rede['text_pos'] = [ele for ele in rede['text_pos'] if ele in consider]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reden_clean = alleReden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_and_parties(reden_clean):\n",
    "    names = []\n",
    "    parties = []\n",
    "    for rede in reden_clean:\n",
    "        if rede['name'] not in names:\n",
    "            names.append(rede['name'])\n",
    "            parties.append(rede['party'])\n",
    "    return names, parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, parties = get_names_and_parties(reden_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_clean(name, reden):\n",
    "    text = ''\n",
    "    nReden = 0\n",
    "    for rede in reden:\n",
    "        if rede['name'] == name:\n",
    "            nReden += 1\n",
    "            text += ' '.join(rede['text_lem'])\n",
    "    return text, nReden\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_von_parla_mit_dict_text(reden, names, parties):\n",
    "    \n",
    "    parlamentarier = []\n",
    "\n",
    "    for count,name in enumerate(names):\n",
    "        hilf = {\n",
    "            'id':count+1,\n",
    "            'name': names[count],\n",
    "            'party': parties[count],\n",
    "            'type': 'parla'\n",
    "        }\n",
    "        \n",
    "        text, nReden = get_text_clean(name, reden) \n",
    "            \n",
    "        hilf.update({'text_lem': text, 'nReden': nReden})\n",
    "        \n",
    "        parlamentarier.append(hilf)\n",
    "    \n",
    "    return parlamentarier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parlamentarier = liste_von_parla_mit_dict_text(reden_clean, names, parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [ parla['text_lem'] for parla in parlamentarier ]\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=(2/781))\n",
    "f_word = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<777x56118 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 867760 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [ parla['text_lem'] for parla in parlamentarier ]\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=(2/781))\n",
    "tf_idf_matrix  = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<777x56118 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 867760 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_von_parla_mit_dict_vec(reden, parlamentarier, X_csr, vectorizer, f_word):\n",
    "    \n",
    "    parlamentarier_vec = []\n",
    "\n",
    "    for count, parla in tqdm(enumerate(parlamentarier)):\n",
    "        hilf = {\n",
    "            'id':count+1,\n",
    "            'name': parla['name'],\n",
    "            'party': parla['party'],\n",
    "            'nReden': parla['nReden'],\n",
    "            'type': parla['type'],\n",
    "            'text': parla['text_lem'],\n",
    "        }\n",
    "        \n",
    "        vec_numbers = np.array(X_csr.getrow(count).toarray()[0])\n",
    "        vec_numbers2 = np.array(f_word.getrow(count).toarray()[0])\n",
    "        \n",
    "        \n",
    "        maxWX = np.argmax(vec_numbers)\n",
    "          \n",
    "        #vec_numbers = vec_numbers/np.linalg.norm(vec_numbers)\n",
    "        \n",
    "        hilf.update({'vec_numbers': vec_numbers})\n",
    "        mfw = list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(np.argmax(vec_numbers2))]\n",
    "        msw = list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(np.argmax(vec_numbers))]\n",
    "        hilf.update({'maxTFIDF': msw, 'mfw': mfw})\n",
    "        \n",
    "        \n",
    "        parlamentarier_vec.append(hilf)\n",
    "    \n",
    "    return parlamentarier_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "777it [00:08, 93.16it/s] \n"
     ]
    }
   ],
   "source": [
    "parlamentarier_vec = liste_von_parla_mit_dict_vec(reden_clean, parlamentarier, tf_idf_matrix.copy(), vectorizer, f_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_similarity = tf_idf_matrix * tf_idf_matrix.T \n",
    "\n",
    "similarity = pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cotop_graph_erstellen(parlamentarier, min_weight, similarity):\n",
    "\n",
    "    graph = {\n",
    "        'directed': False,\n",
    "        'graph': 'semant_graph',\n",
    "        'links': [],\n",
    "        'nodes': parlamentarier,\n",
    "    }\n",
    "\n",
    "    for ix,nodeI in enumerate(graph['nodes']):\n",
    "        for jx,nodeJ in enumerate(graph['nodes']):\n",
    "            if nodeI['id'] < nodeJ['id']:\n",
    "                source = nodeI['id']\n",
    "                target = nodeJ['id']\n",
    "                #weight = cos_sim(nodeI['vec_numbers'], nodeJ['vec_numbers'])\n",
    "                #r = np.corrcoef(nodeI['vec_numbers'], nodeJ['vec_numbers'])\n",
    "                #weight = r[0,1]\n",
    "                weight = similarity[ix,jx]\n",
    "                if weight > min_weight:\n",
    "                    link_dict = {\n",
    "                        'source':source,\n",
    "                        'target':target,\n",
    "                        'weight':weight,\n",
    "                        'type': 'parla_parla'\n",
    "                    }\n",
    "                    graph['links'].append(link_dict)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_parla = cotop_graph_erstellen(parlamentarier_vec, 0, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen des redengraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 19657/19657 [00:00<00:00, 620937.30it/s]\n"
     ]
    }
   ],
   "source": [
    "reden_ohne_text = []\n",
    "for rede in tqdm(alleReden):\n",
    "    hilf = {\n",
    "        'id':rede['id'],\n",
    "        'name':rede['name'],\n",
    "        'party':rede['party'],\n",
    "        'type':'speech'}\n",
    "    reden_ohne_text.append(hilf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [' '.join(rede['text_lem']) for rede in alleReden]\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=(2/len(corpus)))\n",
    "f_word_speech = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<19657x58189 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1861821 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_word_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, min_df=(2/len(corpus)))\n",
    "    tf_idf_matrix  = vectorizer.fit_transform(corpus)\n",
    "    return tf_idf_matrix, vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf, vocab = get_tfidf([' '.join(rede['text_lem']) for rede in alleReden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<19657x58189 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1861821 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(tfidf, Y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vec_a, vec_b):\n",
    "    \n",
    "    return np.dot(vec_a, vec_b) /(np.linalg.norm(vec_a) * np.linalg.norm(vec_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cotop_graph_erstellen(reden_clean, min_weight, similarity, tf_idf_matrix, f_word_speech):\n",
    "    graph = {\n",
    "        'directed': False,\n",
    "        'graph': 'semant_graph_reden_knoten',\n",
    "        'links': [],\n",
    "        'nodes': reden_ohne_text,\n",
    "    }\n",
    "    links = []\n",
    "    n=0\n",
    "    for ix,nodeI in tqdm(enumerate(graph['nodes'])):\n",
    "        msw = list(vocab.keys())[list(vocab.values()).index(np.argmax(tf_idf_matrix.getrow(ix).toarray()[0]))]\n",
    "        mfw = list(vocab.keys())[list(vocab.values()).index(np.argmax(f_word_speech.getrow(ix).toarray()[0]))]\n",
    "        nodeI.update({'msw':msw, 'mfw': mfw})\n",
    "        for jx,nodeJ in enumerate(graph['nodes']):\n",
    "            if ix < jx:\n",
    "                source = nodeI['id']\n",
    "                target = nodeJ['id']\n",
    "                #weight = cos_sim(nodeI['vec_numbers'], nodeJ['vec_numbers'])\n",
    "                #r = np.corrcoef(nodeI['vec_numbers'], nodeJ['vec_numbers'])\n",
    "                #weight = r[0,1]\n",
    "                #weight = similarity.getrow(ix).toarray()[jx]\n",
    "                weight = similarity[ix,jx]\n",
    "                if weight > min_weight:\n",
    "                    link_dict = {\n",
    "                        'source':source,\n",
    "                        'target':target,\n",
    "                        'weight':weight, \n",
    "                        'type':'rede_rede'\n",
    "                    }\n",
    "                    graph['links'].append(link_dict)\n",
    "                else:\n",
    "                    n+=1\n",
    "                    \n",
    "    \n",
    "    return graph, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19657it [06:52, 47.68it/s]\n"
     ]
    }
   ],
   "source": [
    "graph_reden, nicht_drinne = cotop_graph_erstellen(alleReden, 0.2, similarity, tfidf, f_word_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kombinieren der beiden graphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 777/777 [00:05<00:00, 136.45it/s]\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "for node in tqdm(graph_parla['nodes']):\n",
    "    for rede in reden_ohne_text:\n",
    "        if node['name']==rede['name']:\n",
    "            link_dict = {\n",
    "                        'source':node['id'],\n",
    "                        'target':rede['id'],\n",
    "                        'weight':1, \n",
    "                        'type':'parla_rede'\n",
    "                    }\n",
    "            links.append(link_dict)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19657"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes done\n"
     ]
    }
   ],
   "source": [
    "graphforgephi = nx.Graph()\n",
    "for node in graph_reden['nodes']:\n",
    "    node['type'] = 1\n",
    "    graphforgephi.add_node(node['id'], name=node['name'], party=node['party'], typ=node['type'], msw=node['msw'], mfw=node['mfw']);\n",
    "\n",
    "for node in graph_parla['nodes']:\n",
    "    node['type'] = 10\n",
    "    graphforgephi.add_node(node['id'],name = node['name'], party=node['party'], msw=node['maxTFIDF'], mfw = node['mfw'], nReden = node['nReden'], typ=node['type']);\n",
    "\n",
    "print('nodes done')\n",
    "\n",
    "for link in graph_reden['links']:\n",
    "    if link['weight'] > 0: #0.22285057620424778:\n",
    "        graphforgephi.add_edge(link['source'],link['target'],weight = link['weight'], typ=link['type'])\n",
    "        \n",
    "#for link in graph_parla['links']:\n",
    "    #if link['weight'] > 0: #0.22285057620424778:\n",
    "        #graphforgephi.add_edge(link['source'],link['target'],weight = link['weight'], typ=link['type'])\n",
    "        \n",
    "for link in links:\n",
    "    if link['weight'] > 0: #0.22285057620424778:\n",
    "        graphforgephi.add_edge(link['source'],link['target'],weight = link['weight'], typ=link['type'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save done\n"
     ]
    }
   ],
   "source": [
    "nx.write_gexf(graphforgephi, \"graphforgephi_reden_parla.gexf\")\n",
    "print('save done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
