{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funktionen zum Durchsuchen und Filtern der Reden\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Liste enthält 10791 Reden\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "\n",
    "# Das (natural language toolkit) müsst ihr bestimmt installieren.\n",
    "# Weiß noch jemand, wie das geht?\n",
    "import nltk\n",
    "\n",
    "# Die collections.Counter package müsst ihr bestimmt auch installieren.\n",
    "# Die brauchen wir später, um Worte zu zählen.\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Hier legen wir fest, welche Daten (Wahlperiode 19 oder 20) wir laden:\n",
    "legislatur = 20\n",
    "\n",
    "# Wir generieren eine leere Liste:\n",
    "alleReden = []\n",
    "\n",
    "# Wir öffnen den entsprechende File (Dateipfad anpassen!):\n",
    "with jsonlines.open(f'../../data/speeches_{legislatur}.jsonl') as f:\n",
    "    for line in f.iter():\n",
    "    # Wir packen alles Zeile für Zeile zu unserer Liste:\n",
    "        alleReden.append(line)\n",
    "        \n",
    "# Wir sortieren nach Datum:\n",
    "alleReden.sort(key = lambda x:x['date'])\n",
    "\n",
    "# Wir lassen uns zeigen, wie viele Reden enthalten sind.\n",
    "print(f'Die Liste enthält {len(alleReden)} Reden')\n",
    "# Neue Zeile auf der Konsole:\n",
    "print(f'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suchfunktionen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diese Liste (Suche nach letzte Generation) enthält 2 Reden.\n",
      "\n",
      "\n",
      "Diese Liste (Suche nach letzte Generation) enthält 2 Sätze\n",
      "\n",
      "\n",
      "Diese Liste (Suche nach Olaf Scholz) enthält 171 Reden\n",
      "\n",
      "\n",
      "In 4 Reden gibt es 6 Sätze von Olaf Scholz, die Klimakrise enthalten:\n",
      "\n",
      "\n",
      " Satz 1: Die Klimakrise erfordert entschlossenes, systematisches und international abgestimmtes Vorgehen.\n",
      "\n",
      "\n",
      " Satz 2: Deshalb bieten wir China Zusammenarbeit an bei Menschheitsherausforderungen wie der Klimakrise, der Pandemie oder der Rüstungskontrolle.\n",
      "\n",
      "\n",
      " Satz 3: Vor uns liegen enorme Aufgaben: die wirtschaftliche Transformation voranbringen, die Klimakrise in den Griff bekommen, demografischen Wandel gestalten, Frieden in Europa sichern.\n",
      "\n",
      "\n",
      " Satz 4: Aber jetzt ist klar: Die Erneuerbaren brauchen wir nicht nur wegen der Klimakrise.\n",
      "\n",
      "\n",
      " Satz 5: Angesichts von Krieg und Klimakrise geht es heute auch um Europa selbst.\n",
      "\n",
      "\n",
      " Satz 6: Meine Damen und Herren, wir leben in einer besonders herausfordernden Zeit: Die Klimakrise spitzt sich zu; der Krieg ist zurück in Europa; die globalen Gewichte verschieben sich.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Zunächst brauchen wir eine Funktion, die uns die Reden gibt, die ein bestimmtes Wort enthalten. \n",
    "#  Funktion für Textsuche: \n",
    "#  Gibt eine Untermenge an Reden zurück, die einen bestimmten String (Wort) enthalten.\n",
    "\n",
    "def find_speeches_with_word(search_term, speeches):\n",
    "    filtered_speeches = []\n",
    "    for speech in speeches:\n",
    "        if ( search_term in speech['text'] ):\n",
    "            filtered_speeches.append(speech)\n",
    "    return filtered_speeches\n",
    "\n",
    "## Probieren wir das mal aus.\n",
    "#  Das ist unser Suchwort (oder String):\n",
    "wort = 'letzte Generation'\n",
    "#wort = 'Kapital'\n",
    "\n",
    "#  Hier rufen wir die Suchfunktion auf und speichern die Untermenge der Reden.\n",
    "untermenge = find_speeches_with_word(wort, alleReden)\n",
    "\n",
    "# Wir lassen uns zeigen, wie viele Reden enthalten sind.\n",
    "print(f'Diese Liste (Suche nach {wort}) enthält {len(untermenge)} Reden.')\n",
    "# Neue Zeile auf der Konsole:\n",
    "print(f'\\n')\n",
    "\n",
    "## Reden sind lang und die Worte tauchen in verschiedenen Kontexten auf.\n",
    "#  Wir würden gerne alle Sätze sehen, in denen der Suchbegriff vorkommt.\n",
    "#  Aber natürlich kommt unser Suchstring nur in Sätzen vor, die in de Untermenge an Reden sind. \n",
    "\n",
    "def find_sentences_with_word(search_term, speeches):\n",
    "    sents_with_words = []\n",
    "    for speech in speeches:\n",
    "        sent_list = nltk.sent_tokenize(speech['text'])\n",
    "        for sent in sent_list:\n",
    "            if search_term in sent:\n",
    "                sents_with_words.append(sent)\n",
    "    return sents_with_words\n",
    "                \n",
    "# Probieren wir diese Funktion einmal aus:\n",
    "satz_liste = find_sentences_with_word(wort,untermenge)\n",
    "\n",
    "# Wir lassen uns zeigen, wie viele Sätze in der Liste enthalten sind.\n",
    "print(f'Diese Liste (Suche nach {wort}) enthält {len(satz_liste)} Sätze')\n",
    "# Neue Zeile auf der Konsole:\n",
    "print(f'\\n')\n",
    "\n",
    "# Wollen wir uns die alle anzeigen lassen? Ja oder Nein?\n",
    "wir_wollen = False\n",
    "if wir_wollen:\n",
    "    for satz in satz_liste:\n",
    "        print(satz)\n",
    "\n",
    "        \n",
    "## Nun wäre es doch spannend, die Reden einer Partei oder eines Politikers zu sehen.\n",
    "#  Dazu entwickeln wir eine Funktion, die es erlaubt, in den anderen Felder (keys) zu suchen.\n",
    "#  Funktion, mit der man eine Menge an Reden nach verschiedenen Kriterien filtern kann.\n",
    "#  Es wird die entsprechende Untermenge zurückgegeben.\n",
    "#  'what' enthält den Key, wo gesucht werden soll. Interessant vor allem: 'name' und 'party'\n",
    "\n",
    "def filter_speeches_for(what, search_term, speeches):\n",
    "    filtered_speeches = []\n",
    "    for speech in speeches:\n",
    "        if search_term in speech[what]:\n",
    "            filtered_speeches.append(speech)\n",
    "        \n",
    "    filtered_speeches.sort(key = lambda x:x['date'])   \n",
    "    return filtered_speeches\n",
    "\n",
    "# Beispiel: Für alle Reden von Olaf Scholz:\n",
    "suche_nach = 'Olaf Scholz'\n",
    "untermenge = filter_speeches_for('name', suche_nach, alleReden)\n",
    "# Wir lassen uns zeigen, wie viele Reden enthalten sind.\n",
    "print(f'Diese Liste (Suche nach {suche_nach}) enthält {len(untermenge)} Reden')\n",
    "print(f'\\n')\n",
    "\n",
    "####\n",
    "## Jetzt könnten wir die Sätze von Olaf Scholz mit einem bestimmten Wort anschauen.\n",
    "#  Das ist unser Suchwort (oder String):\n",
    "#wort = 'Klimawandel'\n",
    "wort = 'Klimakrise'\n",
    "\n",
    "#  Hier rufen wir die Suchfunktion auf und speichern die Untermenge der Reden.\n",
    "untermenge = find_speeches_with_word(wort, alleReden)\n",
    "\n",
    "# Wir suchen nach:\n",
    "suche_nach = 'Olaf Scholz'\n",
    "untermenge = filter_speeches_for('name', suche_nach, untermenge)\n",
    "satz_liste = find_sentences_with_word(wort,untermenge)\n",
    "\n",
    "print(f'In {len(untermenge)} Reden gibt es {len(satz_liste)} Sätze von {suche_nach}, die {wort} enthalten:')\n",
    "print(f'\\n')\n",
    "\n",
    "# Die schauen wir uns an.\n",
    "for sx,satz in enumerate(satz_liste):\n",
    "    print(f' Satz {sx+1}: {satz}')\n",
    "    print(f'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungen/Hausaufgaben\n",
    "\n",
    "1. Probiert es aus!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fortgeschrittener Teil\n",
    "## Welche Worte kommen häufig vor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 18 Reden gibt es 22 Sätze, die Kapitalismus enthalten:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####\n",
    "## Lass mal was versuchen:\n",
    "wort = 'Kapitalismus'\n",
    "#wort ='Nachhaltigkeit'\n",
    "\n",
    "#  Hier rufen wir die Suchfunktion auf und speichern die Untermenge der Reden.\n",
    "untermenge = find_speeches_with_word(wort, alleReden)\n",
    "satz_liste = find_sentences_with_word(wort,untermenge)\n",
    "\n",
    "print(f'In {len(untermenge)} Reden gibt es {len(satz_liste)} Sätze, die {wort} enthalten:')\n",
    "print(f'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In der Liste befinden sich 246 Worte: \n",
      "\n",
      "[('Kapitalismus', 18), ('Nachhaltigkeit', 4), ('Profite', 2), ('sprechen', 2), ('Menschen', 2), ('Damen', 2), ('Herren', 2), ('Auch', 2), ('Kapitalismuskritik', 2), ('Ihnen', 2), ('Beispiel', 2), ('worden', 2), ('Russland', 2), ('heute', 2), ('mehr', 2), ('Krisen', 2), ('Marx', 2), ('Frage', 1), ('beantworten', 1), ('Weil', 1)]\n"
     ]
    }
   ],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "german_stop_words = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "# Die schauen wir uns an und generieren eine Wortliste\n",
    "wort_liste = []\n",
    "for sx,satz in enumerate(satz_liste):\n",
    "    #print(f' Satz {sx+1}: {satz}')\n",
    "    worte = nltk.word_tokenize(satz)\n",
    "    #print(f' Satz {sx+1}: {worte}')\n",
    "    wort_liste.extend(worte)\n",
    "\n",
    "wort_liste_clean = []\n",
    "for wort in wort_liste:\n",
    "    if wort not in german_stop_words:\n",
    "        if len(wort)>3: # simple shortcut for other \"stop words\"\n",
    "            wort_liste_clean.append(wort)\n",
    "    \n",
    "print(f'In der Liste befinden sich {len(wort_liste_clean)} Worte: \\n')\n",
    "#print(wort_liste_clean)\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter(wort_liste_clean)\n",
    "print(counts.most_common(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Kapitalismus', 18), ('Nachhaltigkeit', 4), ('Krise', 3), ('Profit', 2), ('Mensch', 2), ('Dame', 2), ('Herr', 2), ('Kapitalismuskritik', 2), ('Beispiel', 2), ('Russland', 2), ('Marx', 2), ('Frage', 1), ('Kosten', 1), ('Magazin', 1), ('Spiegel', 1), ('Umfrage', 1), ('Jugendliche', 1), ('Unterstützung', 1), ('Natur', 1), ('Bedürfnis', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Umsetzung mit Spacy\n",
    "import spacy\n",
    "nlp = spacy.load('de') #load spacy model\n",
    "\n",
    "def get_list_of_lemmata(satz_liste):\n",
    "    wort_liste = []\n",
    "    for sx,satz in enumerate(satz_liste):\n",
    "        #print(f' Satz {sx+1}: {satz}')\n",
    "        doc = nlp(satz)\n",
    "        for token in doc:\n",
    "            if token.pos_ in ['NOUN','PROPN']:\n",
    "                #print(token.lemma_)\n",
    "                wort_liste.append(token.lemma_)    \n",
    "    return wort_liste\n",
    "    \n",
    "wort_liste = get_list_of_lemmata(satz_liste)\n",
    "\n",
    "counts = Counter(wort_liste).most_common(20)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
