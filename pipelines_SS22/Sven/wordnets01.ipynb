{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "import re\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('de') #load spacy model\n",
    "\n",
    "# for WordClouds \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "with open(\"../../data/speeches_20_withoutcomments.jsonl\",'r',encoding = \"utf8\") as fp:\n",
    "    data = list(fp)\n",
    "speeches = []\n",
    "for line in data:\n",
    "    speeches.append(json.loads(line))\n",
    "\n",
    "# clean party labels\n",
    "for rede in speeches:\n",
    "    rede['party']=rede['party'].replace(u'\\xa0', u' ')\n",
    "    if rede['party']=='Bündnis 90/Die Grünen':\n",
    "        rede['party']='BÜNDNIS 90/DIE GRÜNEN'\n",
    "    if rede['party']=='Fraktionslos':\n",
    "        rede['party']='fraktionslos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# last changed: 02/06/2022\n",
    "def filter_for(what, search_terms, speeches):\n",
    "    filtered_speeches = []\n",
    "    if what == 'text':\n",
    "        search_terms_low = []\n",
    "        for st in search_terms:\n",
    "            search_terms_low.append(st.lower())\n",
    "        for speech in speeches:\n",
    "            match = [st in speech[what].lower() for st in search_terms_low]\n",
    "            #if all(st == True for st in match):\n",
    "            if any(st in speech[what] for st in search_terms):\n",
    "                #print(match)\n",
    "            #if ( search_terms in speech[what] ):\n",
    "                filtered_speeches.append(speech)\n",
    "    else:\n",
    "        for speech in speeches:\n",
    "            if ( speech[what] in set(search_terms) ):\n",
    "                filtered_speeches.append(speech)\n",
    "        \n",
    "    filtered_speeches.sort(key = lambda x:x['date'])   \n",
    "    return filtered_speeches\n",
    "\n",
    "\n",
    "#focal_terms = ['Digitalisierung','Zusammenhalt','Demokratie']\n",
    "focal_terms = ['Digitalisierung']\n",
    "#focal_terms = ['Plattform','Demokratie']\n",
    "#focal_terms = ['extrem','Plattform']\n",
    "#focal_terms = ['plattform','demokratie']\n",
    "#focal_terms = ['Plattform','Meinung']\n",
    "#focal_terms = ['Netzwerk','Meinung']\n",
    "#focal_terms = ['Volk ','Volk.','Volk!','Volk?']\n",
    "subset = filter_for('text', ['Klima'], speeches)\n",
    "#subset = filter_for('party', ['AfD'], subset)\n",
    "subset = filter_for('party', ['BÜNDNIS 90/DIE GRÜNEN'], subset)\n",
    "\n",
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:07<00:00, 17.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1334"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def neighborhood(iterable):\n",
    "    iterator = iter(iterable)\n",
    "    prev_item = None\n",
    "    current_item = next(iterator)  # throws StopIteration if empty.\n",
    "    for next_item in iterator:\n",
    "        yield (prev_item, current_item, next_item)\n",
    "        prev_item = current_item\n",
    "        current_item = next_item\n",
    "    yield (prev_item, current_item, None)\n",
    "\n",
    "#for prev,item,next in neighborhood(sentences):\n",
    "#    print (prev, item, next )  \n",
    "    \n",
    " \n",
    "sentences = []\n",
    "#focal_terms_sen = ['Netzwerk','sozial']\n",
    "#focal_terms_sen = ['Russland']\n",
    "focal_terms_sen = ['Klima']\n",
    "window = 10\n",
    "for rede in tqdm.tqdm(subset):\n",
    "    doc = nlp(rede[\"text\"])\n",
    "    sents = doc.sents\n",
    "    mysents = []\n",
    "    for sx,sent in enumerate(sents):\n",
    "        mysents.append(sent)\n",
    "    for sx,sent in enumerate(mysents):\n",
    "        #print(sent.text)\n",
    "        if all(ft.lower() in sent.text.lower() for ft in focal_terms_sen):\n",
    "            sentences.append(sent)\n",
    "            if sx > 0 and sx < len(mysents)-1:\n",
    "                sentences.append(mysents[sx-1])\n",
    "                sentences.append(mysents[sx+1])\n",
    "        \n",
    "        \n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lassen Sie uns Klimaschutz und Energiewende als das sehen, was es ist: eine riesengroße Chance für Modernisierung, für Innovation und vor allen Dingen für internationale Solidarität.,\n",
       " Die wiederum hilft uns dabei, viel schneller auf 100 Prozent Erneuerbare zu kommen.,\n",
       " Denn das, was wir hier in Deutschland für Energiewende und Klimaschutz tun, hat enorme Auswirkungen auf den Rest der Welt.,\n",
       " Denn das, was wir hier in Deutschland für Energiewende und Klimaschutz tun, hat enorme Auswirkungen auf den Rest der Welt.,\n",
       " Lassen Sie uns Klimaschutz und Energiewende als das sehen, was es ist: eine riesengroße Chance für Modernisierung, für Innovation und vor allen Dingen für internationale Solidarität.,\n",
       " Die Klimakrise geht bisher vor allem zulasten der Länder im Globalen Süden, die schon jetzt die Auswirkungen am heftigsten spüren.,\n",
       " Die Klimakrise geht bisher vor allem zulasten der Länder im Globalen Süden, die schon jetzt die Auswirkungen am heftigsten spüren.,\n",
       " Denn das, was wir hier in Deutschland für Energiewende und Klimaschutz tun, hat enorme Auswirkungen auf den Rest der Welt.,\n",
       " Sie brauchen dringend Unterstützung bei der Anpassung an die Klimaveränderung, und sie brauchen Gewissheit, dass die Industrienationen beim Klimaschutz vorangehen.,\n",
       " Sie brauchen dringend Unterstützung bei der Anpassung an die Klimaveränderung, und sie brauchen Gewissheit, dass die Industrienationen beim Klimaschutz vorangehen.]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7332"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevantPOS = ['NOUN','ADJ','PROPN']\n",
    "sentencesNN = []\n",
    "words = []\n",
    "for sen in sentences:\n",
    "    lem = []\n",
    "    for token in sen:\n",
    "        if token.pos_ in relevantPOS:\n",
    "            lem.append(token.lemma_)\n",
    "    sentencesNN.append(lem)\n",
    "    words.extend(lem)\n",
    "#sentencesNN\n",
    "len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klimakrise\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zähler = collections.Counter(words).most_common()\n",
    "#print(zähler)\n",
    "\n",
    "words_clean = []\n",
    "#print(focal_terms_sen)\n",
    "for word in words:\n",
    "    #if focal_terms_sen[0] in word or focal_terms[0] in word or 'Herr' in word or 'Dame' in word or 'Kollege' in word or 'Kollegin' in word or \"/\" in word:\n",
    "    if 'Herr' in word or 'Dame' in word or 'Kollege' in word or 'Kollegin' in word or \"/\" in word:\n",
    "        pass\n",
    "    else:\n",
    "        words_clean.append(word)\n",
    "    \n",
    "#print(len(words_clean))\n",
    "zähler = collections.Counter(words_clean).most_common()\n",
    "#print(zähler)\n",
    "\n",
    "num_words = 1000\n",
    "count = 0\n",
    "new_words = []\n",
    "for word in zähler:\n",
    "    #print(word[0])\n",
    "    if count < num_words:\n",
    "        new_words.append(word[0])\n",
    "        if w1 == focal_terms_sen[0]:\n",
    "            wxFocus = count\n",
    "    count = count + 1    \n",
    "\n",
    "print(new_words[wxFocus])\n",
    "len(new_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:00,  5.06it/s]\u001b[A\n",
      "2it [00:00,  5.20it/s]\u001b[A\n",
      "3it [00:00,  5.38it/s]\u001b[A\n",
      "4it [00:00,  5.49it/s]\u001b[A\n",
      "5it [00:00,  5.70it/s]\u001b[A\n",
      "6it [00:01,  5.69it/s]\u001b[A\n",
      "7it [00:01,  5.64it/s]\u001b[A\n",
      "8it [00:01,  5.77it/s]\u001b[A\n",
      "9it [00:01,  5.82it/s]\u001b[A\n",
      "10it [00:01,  5.94it/s]\u001b[A\n",
      "11it [00:01,  5.97it/s]\u001b[A\n",
      "12it [00:02,  5.96it/s]\u001b[A\n",
      "13it [00:02,  5.94it/s]\u001b[A\n",
      "14it [00:02,  5.96it/s]\u001b[A\n",
      "15it [00:02,  6.06it/s]\u001b[A\n",
      "16it [00:02,  6.09it/s]\u001b[A\n",
      "17it [00:02,  6.01it/s]\u001b[A\n",
      "18it [00:03,  5.70it/s]\u001b[A\n",
      "19it [00:03,  5.62it/s]\u001b[A\n",
      "20it [00:03,  5.30it/s]\u001b[A\n",
      "21it [00:03,  5.17it/s]\u001b[A\n",
      "22it [00:03,  5.04it/s]\u001b[A\n",
      "23it [00:04,  4.91it/s]\u001b[A\n",
      "24it [00:04,  5.39it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-d3274960373e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentencesNN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = len(new_words) \n",
    "A = np.ndarray(shape=(n,n), dtype=float)\n",
    "\n",
    "for wx1,w1 in tqdm.tqdm(enumerate(new_words)):\n",
    "    #print(zähler[wx1][0])\n",
    "\n",
    "        \n",
    "    for wx2,w2 in enumerate(new_words):\n",
    "        if(wx2 > wx1):\n",
    "            A[wx1][wx2] = 0\n",
    "            for sen in sentencesNN:\n",
    "                if w1 in sen and w2 in sen:\n",
    "                    \n",
    "                    A[wx1][wx2] = A[wx1][wx2] + 1\n",
    "                    \n",
    "        \n",
    "            normcount = zähler[wx1][1]*zähler[wx2][1]\n",
    "            #if(normcount < 2):\n",
    "            #    print(normcount)\n",
    "            A[wx1][wx2] = A[wx1][wx2]/normcount\n",
    "            A[wx2][wx1] = A[wx1][wx2]            \n",
    "\n",
    "#plt.imshow(A)\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: [0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158\n",
      " 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158\n",
      " 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158\n",
      " 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158\n",
      " 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158\n",
      " 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158\n",
      " 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158\n",
      " 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158\n",
      " 0.05263158 0.05263158 0.05263158 0.03508772 0.03508772 0.03508772\n",
      " 0.02631579 0.02631579 0.02631579 0.02631579 0.02631579 0.02631579\n",
      " 0.02631579 0.02631579 0.02631579 0.02631579 0.02631579 0.02631579\n",
      " 0.02631579 0.02631579 0.02631579 0.02631579]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Nachhaltigkeit',\n",
       " 'Biodiversität',\n",
       " 'Posten',\n",
       " 'Fragezeichen',\n",
       " 'Fiskalausbeutung',\n",
       " 'Hauptrubriken',\n",
       " 'Verplanwirtschaftlichung',\n",
       " 'Umerziehungsstaat',\n",
       " 'Weltwirtschaftskrieg',\n",
       " 'wohlwissend',\n",
       " 'Tier',\n",
       " 'Objekt',\n",
       " 'welthistorisches',\n",
       " 'Ausland',\n",
       " 'Baupolitik',\n",
       " 'Rebranding',\n",
       " 'Nation',\n",
       " 'Credo',\n",
       " 'Gender',\n",
       " 'Stand',\n",
       " 'Kritische',\n",
       " 'bestimmen',\n",
       " 'Theorie',\n",
       " 'Hypothese',\n",
       " 'CO2-Emissionen',\n",
       " 'maßgeblich',\n",
       " 'Benzin',\n",
       " 'eindeutig',\n",
       " 'Beweis',\n",
       " 'Steuerhoheit',\n",
       " 'Deckmantel',\n",
       " 'demokratiefeindlichen',\n",
       " 'Gesundheit',\n",
       " 'verlässliches',\n",
       " 'Neuzuweisungen',\n",
       " 'bewährt',\n",
       " 'Corona',\n",
       " 'Interessenpolitik',\n",
       " 'globalisierter',\n",
       " 'zeitgemäß',\n",
       " 'vertrauensvoll',\n",
       " 'Panikrhetorik',\n",
       " 'Regelungswut',\n",
       " 'Pump',\n",
       " 'Subunternehmerin',\n",
       " 'bloße',\n",
       " 'Klima-und-Transformations-Sondervermögen',\n",
       " 'Kardinalpunkten',\n",
       " 'Verbrauch',\n",
       " 'Geldbeutel',\n",
       " 'Panikmodus',\n",
       " 'Pflanze',\n",
       " 'Rücklagen',\n",
       " 'menschengemachten',\n",
       " 'Kohle',\n",
       " 'Wähler',\n",
       " 'Zug',\n",
       " 'Sprache',\n",
       " 'Grundrechte',\n",
       " 'drastisch',\n",
       " 'stark',\n",
       " 'Wende',\n",
       " 'Tausende',\n",
       " 'einzig',\n",
       " 'wissenschaftlich',\n",
       " 'China',\n",
       " 'weit',\n",
       " 'Strom',\n",
       " 'Richtung',\n",
       " 'Wissenschaftler']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numWords = 70\n",
    "final_words = []\n",
    "\n",
    "# Numpy Array\n",
    "scores = A[wxFocus]\n",
    "# Getting indices of N = 3 maximum values\n",
    "x = np.argsort(scores)[::-1][:numWords]\n",
    "#print(\"Indices:\",x)\n",
    "# Getting N maximum values\n",
    "print(\"Values:\",scores[x])\n",
    "\n",
    "for ele in x:\n",
    "    final_words.append( new_words[ele] )\n",
    "\n",
    "final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321 70\n"
     ]
    }
   ],
   "source": [
    "nodes=[]\n",
    "curid=1\n",
    "for word in final_words:\n",
    "    node = {\n",
    "    'id' :  curid,\n",
    "    'name' : word\n",
    "    }\n",
    "    nodes.append(node)\n",
    "    curid=curid+1\n",
    "    \n",
    "graph = {\n",
    "    'directed': False,\n",
    "    'graph': 'word_graph',\n",
    "    'links': [],\n",
    "    'nodes': nodes\n",
    "}\n",
    "\n",
    "links = []\n",
    "linkedwords =[]\n",
    "linkedids =[]\n",
    "lx = 0;\n",
    "for wx1,w1 in enumerate(final_words):\n",
    "    #print(wx1)\n",
    "    for wx2,w2 in enumerate(final_words):\n",
    "        if(wx2 > wx1) and A[wx1][wx2]>0.001:\n",
    "            link_dict = {\n",
    "                    'source':wx1+1,\n",
    "                    'target':wx2+1,\n",
    "                    'weight': A[wx1][wx2]      \n",
    "                    }\n",
    "            graph['links'].append(link_dict)\n",
    "                      \n",
    "                    #for link in links:\n",
    "\n",
    "\n",
    "#graph['links']=links\n",
    "#print(zähler)\n",
    "print(len(graph['links']),\n",
    "len(graph['nodes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for link in graph['links']:\n",
    "#    if link['weight']>1:\n",
    "        #print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 110210.69it/s]\n",
      "100%|██████████| 321/321 [00:00<00:00, 291296.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes done\n",
      "links done\n",
      "save done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "graphforgephi = nx.Graph()\n",
    "for node in tqdm.tqdm(graph['nodes']):\n",
    "    graphforgephi.add_node(node['id'],name = node['name']);\n",
    "print('nodes done')\n",
    "for link in tqdm.tqdm(graph['links']):\n",
    "    #weight = all((' '.join([w1['name'],w2['name']]) in linkedwords)\n",
    "    #print(weight)         \n",
    "    graphforgephi.add_edge(link['source'],link['target'],weight=link['weight'])\n",
    "print('links done')   \n",
    "nx.write_gexf(graphforgephi, \"graphforgephi.gexf\")\n",
    "print('save done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            for sen in sentencesNN:\n",
    "                if w1['name'] in sen and w2['name'] in sen:\n",
    "                    weight = len([ele for ele in linkedwords if ele == (' '.join([w1['name'],w2['name']]))])\n",
    "                    #    links[]\n",
    "                    #else:\n",
    "                    #print(weight)\n",
    "                    linkedwords.append(' '.join([w1['name'],w2['name']]))\n",
    "                    #linkedids[ w1['id'] ] , w2['id'] )\n",
    "                    link_dict = {\n",
    "                    'source':w1['id'],\n",
    "                    'target':w2['id'],\n",
    "                    'sourceWD':w1['name'],\n",
    "                    'targetWD':w2['name'],\n",
    "                    'weight': weight+1      \n",
    "                    }\n",
    "                    #print(link_dict)\n",
    "                    links.append(link_dict)\n",
    "                    graph['links'].append(link_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
